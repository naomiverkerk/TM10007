{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to use from colab environment\n",
    "#!pip install -q --upgrade git+https://github.com/naomiverkerk/TM10007.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "from sklearn import feature_selection \n",
    "from sklearn import preprocessing\n",
    "from sklearn import neighbors\n",
    "from sklearn import svm\n",
    "from sklearn import decomposition\n",
    "from load_data import load_data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Classifiers and kernels\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Loading Data\n",
    "data = load_data() \n",
    "X = data\n",
    "X = X.replace(np.inf, np.nan)\n",
    "Y = data['label']\n",
    "del X['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duplicate check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates values\n",
    "data.drop_duplicates(keep='first')\n",
    "print(f'Number of duplicated datasets = {data.duplicated().sum()}')\n",
    "\n",
    "# Check for duplicates columns\n",
    "data.columns.drop_duplicates(keep='first')\n",
    "print(f'Number of duplicated features = {data.columns.duplicated().sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_count = Y.value_counts()\n",
    "z = np.arange(len(label_count.index))\n",
    "width = 0.75\n",
    "fig, ax = plt.subplots(figsize = (6,5.5))\n",
    "count1 = ax.bar(0.5, label_count.values[0], width, label=label_count.index[0])\n",
    "count2 = ax.bar(1.5, label_count.values[1], width, label=label_count.index[1])\n",
    "\n",
    "ax.set_ylabel('Scores', fontsize = 15)\n",
    "ax.set_title('Number patients per label  ', fontsize = 16)\n",
    "ax.set_xticks([0.5,1.5], list(label_count.index), fontsize = 13)\n",
    "ax.legend()\n",
    "ax.bar_label(count1, padding=3)\n",
    "ax.bar_label(count2, padding=3)\n",
    "ax.grid(False)\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split\n",
    "X_train, X_test, Y_train, Y_test= train_test_split(X, Y, test_size = 0.2, random_state = 4, stratify = Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fitter import Fitter, get_common_distributions, get_distributions\n",
    "fi = []\n",
    "\n",
    "col = X.columns\n",
    "for i in (range(0,X.shape[1],1)):\n",
    "    try:\n",
    "        X_tr = X[col[i]].values\n",
    "        f = Fitter(X_tr,\n",
    "            distributions=['gamma',\n",
    "                            'lognorm',\n",
    "                            \"beta\",\n",
    "                            \"burr\",\n",
    "                            \"norm\"])\n",
    "        f.fit()\n",
    "        fi.append(list(pd.DataFrame(f.get_best(method = 'sumsquare_error')).columns))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "flat_list = [item for sublist in fi for item in sublist]\n",
    "from collections import Counter\n",
    "Counter(flat_list).most_common()       \n",
    "from sklearn import preprocessing\n",
    "import seaborn as sns\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "y_10 = le.fit_transform(Y)\n",
    "Y_train_10 = le.fit_transform(Y_train)\n",
    "Y_test_10 = le.fit_transform(Y_test)\n",
    "# classes = data.label\n",
    "# classes = list(classes.unique())\n",
    "colour = sns.color_palette(\"Set2\")\n",
    "color1=colour[0]\n",
    "color2=colour[1]\n",
    "\n",
    "colormap = np.array([color1,color2])\n",
    "\n",
    "fig = plt.figure(figsize=(24,8))\n",
    "ax = fig.add_subplot(131)\n",
    "ax.set_title(f\"Brats, entire dataset = {len(data)}\", fontsize=15)\n",
    "ax.scatter(data.iloc[:,0], data.iloc[:,1], marker='o', c=colormap[y_10],\n",
    "           s=45, edgecolor='k', cmap=plt.cm.Paired)\n",
    "\n",
    "ax = fig.add_subplot(132)\n",
    "ax.set_title(f\"Training data = {len(X_train)}\", fontsize=15)\n",
    "ax.scatter(X_train.iloc[:,0], X_train.iloc[:,1], marker='o', c=colormap[Y_train_10],\n",
    "           s=45, edgecolor='k', cmap=plt.cm.Paired)\n",
    "\n",
    "ax = fig.add_subplot(133)\n",
    "ax.set_title(f\"Test data= {len(X_test)}\", fontsize=15)\n",
    "ax.scatter(X_test.iloc[:,0], X_test.iloc[:,1], marker='o', c=colormap[Y_test_10],\n",
    "           s=45, edgecolor='k', cmap=plt.cm.Paired)   \n",
    "\n",
    "GBM_patch = mpatches.Patch(color=colour[0], label='GBM')\n",
    "LGG_patch = mpatches.Patch(color=colour[1], label='LGG')\n",
    "fig.legend(handles=[GBM_patch, LGG_patch],loc=\"center right\", prop={'size': 12})\n",
    "fig.subplots_adjust(right=0.95)\n",
    "\n",
    "plt.show()          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Features weghalen met teveel missing values\n",
    "acceptabele_ratio = 0.5\n",
    "train_size = len(X_train.index)\n",
    "removal_rate = round(train_size*acceptabele_ratio)\n",
    "\n",
    "X_train = X_train.dropna(axis=1, thresh=removal_rate)\n",
    "common_cols = list(set(X_train.columns).intersection(X_test.columns))\n",
    "X_test = X_test[common_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imputation met median\n",
    "X_train_missing_median = X_train.fillna(X_train.median())\n",
    "X_train_missing_median = np.nan_to_num(X_train_missing_median)\n",
    "X_test_missing_median = X_test.fillna(X_test.median())\n",
    "X_test_missing_median = np.nan_to_num(X_test_missing_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scaling\n",
    "scaler = preprocessing.RobustScaler()\n",
    "scaler.fit(X_train_missing_median)\n",
    "X_train_scaled = scaler.transform(X_train_missing_median)\n",
    "X_train_scaled = np.nan_to_num(X_train_scaled)\n",
    "X_test_scaled = scaler.transform(X_test_missing_median)\n",
    "X_test_scaled = np.nan_to_num(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.94191425 0.97492092 0.98729222 0.9899666  0.99189023]\n"
     ]
    }
   ],
   "source": [
    "## PCA\n",
    "pca = decomposition.PCA(n_components=0.99, svd_solver= 'full')\n",
    "pca.fit(X_train_scaled)\n",
    "X_train_final = pca.transform(X_train_scaled)\n",
    "X_test_final = pca.transform(X_test_scaled)\n",
    "explained_variance = np.cumsum(pca.explained_variance_ratio_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def randomforest(X_train_final, Y_train_final):\n",
    "\n",
    "    cv_20fold = model_selection.StratifiedKFold(n_splits=5)\n",
    "    class_names = ['GBM', 'LGG']\n",
    "    feature_names = list(X_train.columns)\n",
    "    results = []\n",
    "    best_min_samples_split = []\n",
    "\n",
    "    for validation_index, test_index in cv_20fold.split(X_train_final, Y_train_final):\n",
    "\n",
    "        # Split the data properly\n",
    "        X_validation = X_train_final[validation_index]\n",
    "        y_validation = Y_train_final[validation_index]\n",
    "        \n",
    "        X_test = X_train_final[test_index]\n",
    "        y_test = Y_train_final[test_index]\n",
    "\n",
    "        parameters = {\n",
    "                        \"criterion\": ['gini', 'entropy'],\n",
    "                        \"min_samples_split\": list(range(2,40,2)),\n",
    "                        # \"min_impurity_decrease\": [0,1,2,3,4,5],\n",
    "                        \"max_features\": [1,2,3,4,5],\n",
    "                        \"min_samples_leaf\": list(range(1,20,2)),\n",
    "        }\n",
    "        clf = RandomForestClassifier()\n",
    "        cv_10fold = model_selection.StratifiedKFold(n_splits=5)\n",
    "        grid_search = model_selection.GridSearchCV(clf, parameters, cv=cv_10fold, scoring='roc_auc')\n",
    "        grid_search.fit(X_validation, y_validation)\n",
    "        \n",
    "        # Get resulting classifier\n",
    "        clf = grid_search.best_estimator_\n",
    "        print(f'Best classifier for criterion={clf.criterion} & min_samples_split={clf.min_samples_split} & max_features={clf.max_features} & min_samples_leaf={clf.min_samples_leaf}')\n",
    "\n",
    "        probabilities = clf.predict_proba(X_test)\n",
    "        scores = probabilities[:, 1]\n",
    "        \n",
    "        auc = metrics.roc_auc_score(y_test, scores)\n",
    "        results.append({\n",
    "            'auc': auc,\n",
    "            'criterion': clf.criterion,\n",
    "            'min_samples_split': clf.min_samples_split,\n",
    "            'max_features': clf.max_features,\n",
    "            \"min_samples_leaf\": clf.min_samples_leaf,\n",
    "            'set': 'test'\n",
    "        })\n",
    "        \n",
    "        probabilities_validation = clf.predict_proba(X_validation)\n",
    "        scores_validation = probabilities_validation[:, 1]\n",
    "\n",
    "        # Get the auc\n",
    "        auc_validation = metrics.roc_auc_score(y_validation, scores_validation)\n",
    "        results.append({\n",
    "            'auc': auc_validation,\n",
    "            'criterion': clf.criterion,\n",
    "            'min_samples_split': clf.min_samples_split,\n",
    "            'max_features': clf.max_features,\n",
    "            \"min_samples_leaf\": clf.min_samples_leaf,\n",
    "            'set': 'validation'\n",
    "        })\n",
    "\n",
    "    # plt.figure(dpi=250)\n",
    "    # plot_tree(clf, filled=True,\n",
    "    #             class_names=class_names,\n",
    "    #             feature_names=feature_names,\n",
    "    #             );\n",
    "\n",
    "\n",
    "\n",
    "    results = pd.DataFrame(results)\n",
    "    sns.boxplot(y='auc', x='set', data=results)\n",
    "\n",
    "    p = list(parameters.keys())\n",
    "    optimal_parameter = []\n",
    "    # print(f\"The optimal N={optimal_n}\")\n",
    "\n",
    "    parameter_keys = list(parameters.keys())\n",
    "    for item in parameter_keys:\n",
    "        best_item = [] \n",
    "        for i in list(range(0,10,2)):\n",
    "            best_item.append(results[item][i])\n",
    "\n",
    "        optimal_parameter.append(statistics.median(best_item))\n",
    "        print(f\"The optimal {item}={optimal_parameter[-1]}\")\n",
    "  \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-nearest neighbour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cv_20fold = model_selection.StratifiedKFold(n_splits=5)\n",
    "results = []\n",
    "best_n_neighbors = []\n",
    "n_components = []\n",
    "\n",
    "\n",
    "for validation_index, test_index in cv_20fold.split(X_train_final, Y_train):\n",
    "    # Split the data properly\n",
    "    X_validation = X_train_final[validation_index]\n",
    "    y_validation = Y_train[validation_index]\n",
    "\n",
    "    X_test = X_train_final[test_index]\n",
    "    y_test = Y_train[test_index]\n",
    "\n",
    "    # Create a grid search to find the optimal k using a gridsearch and 10-fold cross validation\n",
    "    # Same as above\n",
    "    parameters = {\n",
    "                    \"n_neighbors\": list(range(1, 26, 2)),\n",
    "                    \"weights\": [\"uniform\", \"distance\"],\n",
    "                    \"metric\": [\"euclidean\", \"manhattan\", \"chebyshev\", \"minkowski\"]\n",
    "\n",
    "    }\n",
    "    \n",
    "    knn = neighbors.KNeighborsClassifier()\n",
    "    cv_10fold = model_selection.StratifiedKFold(n_splits=5)\n",
    "    grid_search = model_selection.GridSearchCV(knn, parameters, cv=cv_10fold, scoring='roc_auc')\n",
    "    grid_search.fit(X_validation, y_validation)\n",
    "    \n",
    "    # Get resulting classifier\n",
    "    clf = grid_search.best_estimator_\n",
    "\n",
    "    # Test the classifier on the test data\n",
    "    probabilities = clf.predict_proba(X_test)\n",
    "    scores = probabilities[:, 1]\n",
    "    \n",
    "    # Get the auc\n",
    "    auc = metrics.roc_auc_score(y_test, scores)\n",
    "    results.append({\n",
    "        'auc': auc,\n",
    "        'k': clf.n_neighbors,\n",
    "        \"weights\": clf.weights,\n",
    "        \"metric\": clf.metric,\n",
    "        'set': 'test'\n",
    "    })\n",
    "    \n",
    "    # Test the classifier on the validation data\n",
    "    probabilities_validation = clf.predict_proba(X_validation)\n",
    "    scores_validation = probabilities_validation[:, 1]\n",
    "    \n",
    "    # Get the auc\n",
    "    auc_validation = metrics.roc_auc_score(y_validation, scores_validation)\n",
    "    results.append({\n",
    "        'auc': auc_validation,\n",
    "        'k': clf.n_neighbors,\n",
    "        \"weights\": clf.weights,\n",
    "        \"metric\": clf.metric,\n",
    "        'set': 'validation'\n",
    "    })\n",
    "    \n",
    "# Create results dataframe and plot it\n",
    "results = pd.DataFrame(results)\n",
    "sns.boxplot(y='auc', x='set', data=results)\n",
    "\n",
    "optimal_parameter = []\n",
    "parameter_keys = list(parameters.keys())\n",
    "\n",
    "for item in parameter_keys:\n",
    "    best_item = [] \n",
    "    for i in list(range(0,10,2)):\n",
    "        best_item.append(results[item][i])\n",
    "    \n",
    "    optimal_parameter.append(statistics.median(best_item))\n",
    "    print(f\"The optimal {item}={optimal_parameter[-1]}\")\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import seaborn as sns\n",
    "import statistics\n",
    "# Create a 20 fold stratified CV iterator\n",
    "\n",
    "cv_20fold = model_selection.StratifiedKFold(n_splits=5)\n",
    "results = []\n",
    "best_n_neighbors = []\n",
    "n_components = []\n",
    "\n",
    "\n",
    "# Loop over the folds\n",
    "for validation_index, test_index in cv_20fold.split(X_train_final, Y_train_final):\n",
    "    # Split the data properly\n",
    "    X_validation = X_train_final[validation_index]\n",
    "    y_validation = Y_train_final[validation_index]\n",
    "\n",
    "    X_test = X_train_final[test_index]\n",
    "    y_test = Y_train_final[test_index]\n",
    "\n",
    "    parameters = {\n",
    "                    \"solver\": ['newton-cg', 'lbfgs', 'liblinear'],\n",
    "                    \"C\": [100, 10, 1.0, 0.1, 0.01],\n",
    "    }\n",
    "\n",
    "    lg = LogisticRegression()\n",
    "    cv_10fold = model_selection.StratifiedKFold(n_splits=5)\n",
    "    grid_search = model_selection.GridSearchCV(lg, parameters, cv=cv_10fold, scoring='roc_auc')\n",
    "    grid_search.fit(X_validation, y_validation)\n",
    "\n",
    "    clf = grid_search.best_estimator_\n",
    "\n",
    "    # Test the classifier on the test data\n",
    "    probabilities = clf.predict_proba(X_test)\n",
    "    scores = probabilities[:, 1]\n",
    "    \n",
    "    # Get the auc\n",
    "    auc = metrics.roc_auc_score(y_test, scores)\n",
    "    results.append({\n",
    "        'auc': auc,\n",
    "        \"solver\": clf.solver,\n",
    "        \"C\": clf.C,\n",
    "        'set': 'test'\n",
    "    })\n",
    "    \n",
    "    # Test the classifier on the validation data\n",
    "    probabilities_validation = clf.predict_proba(X_validation)\n",
    "    scores_validation = probabilities_validation[:, 1]\n",
    "    \n",
    "    # Get the auc\n",
    "    auc_validation = metrics.roc_auc_score(y_validation, scores_validation)\n",
    "    results.append({\n",
    "        'auc': auc,\n",
    "        \"solver\": clf.solver,\n",
    "        \"C\": clf.C,\n",
    "        'set': 'validation'\n",
    "    })\n",
    "    \n",
    "# Create results dataframe and plot it\n",
    "results = pd.DataFrame(results)\n",
    "sns.boxplot(y='auc', x='set', data=results)\n",
    "\n",
    "optimal_parameter = []\n",
    "parameter_keys = list(parameters.keys())\n",
    "\n",
    "for item in parameter_keys:\n",
    "    best_item = [] \n",
    "    for i in list(range(0,10,2)):\n",
    "        best_item.append(results[item][i])\n",
    "    \n",
    "    optimal_parameter.append(statistics.median(best_item))\n",
    "    print(f\"The optimal {item}={optimal_parameter[-1]}\")\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naives Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "# # model = MultinomialNB().fit(X_train, y_train)\n",
    "\n",
    "best_var_smoothing = []\n",
    "results = []\n",
    "\n",
    "cv_20fold = model_selection.StratifiedKFold(n_splits=5)\n",
    "\n",
    "# Loop over the folds\n",
    "for validation_index, test_index in cv_20fold.split(X_train_final, Y_train_final):\n",
    "\n",
    "    # Split the data properly\n",
    "    X_validation = X_train_final[validation_index]\n",
    "    y_validation = Y_train_final[validation_index]\n",
    "    \n",
    "    X_test = X_train_final[test_index]\n",
    "    y_test = Y_train_final[test_index]\n",
    "\n",
    "    parameters = {\"var_smoothing\": list(np.logspace(0,-9, num=101))}\n",
    "    gnb = GaussianNB()\n",
    "    cv_10fold = model_selection.StratifiedKFold(n_splits=5)\n",
    "    grid_search = model_selection.GridSearchCV(gnb, parameters, cv=cv_10fold, scoring='roc_auc')\n",
    "    grid_search.fit(X_validation, y_validation)\n",
    "    \n",
    "    # Get resulting classifier\n",
    "    clf = grid_search.best_estimator_\n",
    "    print(f'Best classifier for parameter={clf.var_smoothing}')\n",
    "    best_var_smoothing.append(clf.var_smoothing)\n",
    "    \n",
    "    # Test the classifier on the test data\n",
    "    probabilities = clf.predict_proba(X_test)\n",
    "    scores = probabilities[:, 1]\n",
    "    \n",
    "    # Get the auc\n",
    "    auc = metrics.roc_auc_score(y_test, scores)\n",
    "    results.append({\n",
    "        'auc': auc,\n",
    "        'k': clf.var_smoothing,\n",
    "        'set': 'test'\n",
    "    })\n",
    "    \n",
    "    # Test the classifier on the validation data\n",
    "    probabilities_validation = clf.predict_proba(X_validation)\n",
    "    scores_validation = probabilities_validation[:, 1]\n",
    "    \n",
    "    # Get the auc\n",
    "    auc_validation = metrics.roc_auc_score(y_validation, scores_validation)\n",
    "    results.append({\n",
    "        'auc': auc_validation,\n",
    "        'k': clf.var_smoothing,\n",
    "        'set': 'validation'\n",
    "    })\n",
    "    \n",
    "# Create results dataframe and plot it\n",
    "results = pd.DataFrame(results)\n",
    "sns.boxplot(y='auc', x='set', data=results)\n",
    "\n",
    "optimal_var_smoothing = (np.median(best_var_smoothing))\n",
    "print(f\"The optimal value of parameter={optimal_var_smoothing}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "import statistics\n",
    "# Create a 20 fold stratified CV iterator\n",
    "\n",
    "cv_20fold = model_selection.StratifiedKFold(n_splits=5)\n",
    "results = []\n",
    "best_n_neighbors = []\n",
    "n_components = []\n",
    "\n",
    "# Loop over the folds\n",
    "for validation_index, test_index in cv_20fold.split(X_train_final, Y_train_final):\n",
    "    # Split the data properly\n",
    "    X_validation = X_train_final[validation_index]\n",
    "    y_validation = Y_train_final[validation_index]\n",
    "\n",
    "    X_test = X_train_final[test_index]\n",
    "    y_test = Y_train_final[test_index]\n",
    "\n",
    "    parameters = {\n",
    "                    'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)],\n",
    "                    'activation': ['tanh', 'relu'],\n",
    "                    'solver': ['sgd', 'adam'],\n",
    "                    'alpha': [0.0001, 0.05],\n",
    "                    'learning_rate': ['constant','adaptive'],\n",
    "    }\n",
    "\n",
    "    clf = MLPClassifier(random_state = 1)\n",
    "    cv_10fold = model_selection.StratifiedKFold(n_splits=5)\n",
    "    grid_search = model_selection.GridSearchCV(clf, parameters, cv=cv_10fold, scoring='roc_auc')\n",
    "    grid_search.fit(X_validation, y_validation)\n",
    "\n",
    "    clf = grid_search.best_estimator_\n",
    "\n",
    "    # Test the classifier on the test data\n",
    "    probabilities = clf.predict_proba(X_test)\n",
    "    scores = probabilities[:, 1]\n",
    "    \n",
    "    # Get the auc\n",
    "    auc = metrics.roc_auc_score(y_test, scores)\n",
    "    results.append({\n",
    "        'auc': auc,\n",
    "        \"hidden layer sizes\": clf.hidden_layer_size,\n",
    "        \"activation\": clf.activation,\n",
    "        \"solver\": clf.solver,\n",
    "        \"alpha\": clf.alpha,\n",
    "        \"learning rate\": clf.learning_rate,\n",
    "        'set': 'test'\n",
    "    })\n",
    "    \n",
    "    # Test the classifier on the validation data\n",
    "    probabilities_validation = clf.predict_proba(X_validation)\n",
    "    scores_validation = probabilities_validation[:, 1]\n",
    "    \n",
    "    # Get the auc\n",
    "    auc_validation = metrics.roc_auc_score(y_validation, scores_validation)\n",
    "    results.append({\n",
    "        'auc': auc,\n",
    "        \"hidden layer sizes\": clf.hidden_layer_size,\n",
    "        \"activation\": clf.activation,\n",
    "        \"solver\": clf.solver,\n",
    "        \"alpha\": clf.alpha,\n",
    "        \"learning rate\": clf.learning_rate,\n",
    "        'set': 'validation'\n",
    "    })\n",
    "    \n",
    "# Create results dataframe and plot it\n",
    "results = pd.DataFrame(results)\n",
    "sns.boxplot(y='auc', x='set', data=results)\n",
    "\n",
    "optimal_parameter = []\n",
    "parameter_keys = list(parameters.keys())\n",
    "\n",
    "for item in parameter_keys:\n",
    "    best_item = [] \n",
    "    for i in list(range(0,10,2)):\n",
    "        best_item.append(results[item][i])\n",
    "    \n",
    "    optimal_parameter.append(statistics.median(best_item))\n",
    "    print(f\"The optimal {item}={optimal_parameter[-1]}\")\n",
    "\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "99c346a2273ffdba9d13565a7efa8a03dbcb19c3f8c63eac036f68118855a3ce"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
