{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to use from colab environment\n",
    "#!pip install -q --upgrade git+https://github.com/naomiverkerk/TM10007.git\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn import svm\n",
    "from sklearn import decomposition\n",
    "import statistics\n",
    "\n",
    "# Preprocessing\n",
    "from load_data import load_data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import RobustScaler, LabelEncoder\n",
    "\n",
    "# Classifiers and kernels\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector as selector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\judit\\Documents\\TM1\\TM10007\\Groepsopdracht\\TM10007\\load_data.py:12: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data = data.append(data2)\n"
     ]
    }
   ],
   "source": [
    "##Loading Data\n",
    "data = load_data()\n",
    "X = data\n",
    "X = X.replace(np.inf, np.nan)\n",
    "Y = data['label']\n",
    "del X['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duplicate check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicated datasets = 0\n",
      "Number of duplicated features = 0\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates values\n",
    "data.drop_duplicates(keep='first')\n",
    "print(f'Number of duplicated datasets = {data.duplicated().sum()}')\n",
    "\n",
    "# Check for duplicates columns\n",
    "data.columns.drop_duplicates(keep='first')\n",
    "print(f'Number of duplicated features = {data.columns.duplicated().sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x396 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "label_count = Y.value_counts()\n",
    "z = np.arange(len(label_count.index))\n",
    "width = 0.75\n",
    "fig, ax = plt.subplots(figsize = (6,5.5))\n",
    "count1 = ax.bar(0.5, label_count.values[0], width, label=label_count.index[0])\n",
    "count2 = ax.bar(1.5, label_count.values[1], width, label=label_count.index[1])\n",
    "\n",
    "ax.set_ylabel('Scores', fontsize = 15)\n",
    "ax.set_title('Number patients per label  ', fontsize = 16)\n",
    "ax.set_xticks([0.5,1.5], list(label_count.index), fontsize = 13)\n",
    "ax.legend()\n",
    "ax.bar_label(count1, padding=3)\n",
    "ax.bar_label(count2, padding=3)\n",
    "ax.grid(False)\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into test and train(design)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split\n",
    "X_train, X_test, Y_train, Y_test= train_test_split(X, Y, test_size = 0.2, random_state = 4, stratify = Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plaatjes, weg?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Nog ff naar kijken of het werkt\n",
    "# from fitter import Fitter, get_common_distributions, get_distributions\n",
    "# fi = []\n",
    "\n",
    "# col = X.columns\n",
    "# for i in (range(0,X.shape[1],1)):\n",
    "#     try:\n",
    "#         X_tr = X[col[i]].values\n",
    "#         f = Fitter(X_tr,\n",
    "#             distributions=['gamma',\n",
    "#                             'lognorm',\n",
    "#                             \"beta\",\n",
    "#                             \"burr\",\n",
    "#                             \"norm\"])\n",
    "#         f.fit()\n",
    "#         fi.append(list(pd.DataFrame(f.get_best(method = 'sumsquare_error')).columns))\n",
    "#     except Exception:\n",
    "#         pass\n",
    "\n",
    "# flat_list = [item for sublist in fi for item in sublist]\n",
    "# from collections import Counter\n",
    "# Counter(flat_list).most_common()       \n",
    "# from sklearn import preprocessing\n",
    "# import seaborn as sns\n",
    "# import matplotlib.patches as mpatches\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# le = preprocessing.LabelEncoder()\n",
    "\n",
    "# y_10 = le.fit_transform(Y)\n",
    "# Y_train_10 = le.fit_transform(Y_train)\n",
    "# Y_test_10 = le.fit_transform(Y_test)\n",
    "# # classes = data.label\n",
    "# # classes = list(classes.unique())\n",
    "# colour = sns.color_palette(\"Set2\")\n",
    "# color1=colour[0]\n",
    "# color2=colour[1]\n",
    "\n",
    "# colormap = np.array([color1,color2])\n",
    "\n",
    "# fig = plt.figure(figsize=(24,8))\n",
    "# ax = fig.add_subplot(131)\n",
    "# ax.set_title(f\"Brats, entire dataset = {len(data)}\", fontsize=15)\n",
    "# ax.scatter(data.iloc[:,0], data.iloc[:,1], marker='o', c=colormap[y_10],\n",
    "#            s=45, edgecolor='k', cmap=plt.cm.Paired)\n",
    "\n",
    "# ax = fig.add_subplot(132)\n",
    "# ax.set_title(f\"Training data = {len(X_train)}\", fontsize=15)\n",
    "# ax.scatter(X_train.iloc[:,0], X_train.iloc[:,1], marker='o', c=colormap[Y_train_10],\n",
    "#            s=45, edgecolor='k', cmap=plt.cm.Paired)\n",
    "\n",
    "# ax = fig.add_subplot(133)\n",
    "# ax.set_title(f\"Test data= {len(X_test)}\", fontsize=15)\n",
    "# ax.scatter(X_test.iloc[:,0], X_test.iloc[:,1], marker='o', c=colormap[Y_test_10],\n",
    "#            s=45, edgecolor='k', cmap=plt.cm.Paired)   \n",
    "\n",
    "# GBM_patch = mpatches.Patch(color=colour[0], label='GBM')\n",
    "# LGG_patch = mpatches.Patch(color=colour[1], label='LGG')\n",
    "# fig.legend(handles=[GBM_patch, LGG_patch],loc=\"center right\", prop={'size': 12})\n",
    "# fig.subplots_adjust(right=0.95)\n",
    "\n",
    "# plt.show()          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(133, 704)\n",
      "(34, 704)\n"
     ]
    }
   ],
   "source": [
    "## Features weghalen met teveel missing values\n",
    "acceptabele_ratio = 0.5\n",
    "train_size = len(X_train.index)\n",
    "removal_rate = round(train_size*acceptabele_ratio)\n",
    "\n",
    "X_train = X_train.dropna(axis=1, thresh=removal_rate)\n",
    "common_cols = list(set(X_train.columns).intersection(X_test.columns))\n",
    "X_test = X_test[common_cols]\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation with KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = KNNImputer()\n",
    "imputer.fit(X_train)\n",
    "X_train = imputer.transform(X_train)\n",
    "X_test = imputer.transform(X_test)\n",
    "\n",
    "# np.isnan(X_train_imputer).sum()\n",
    "# np.isnan(X_test_imputer).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scaling\n",
    "scaler = RobustScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label encoder for scaling Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## scaling van y\n",
    "scaler = LabelEncoder()\n",
    "scaler.fit(Y_train)\n",
    "Y_train = scaler.transform(Y_train)\n",
    "Y_test = scaler.transform(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.94938451 0.9824626  0.98936905 0.99161961]\n"
     ]
    }
   ],
   "source": [
    "## PCA\n",
    "pca = decomposition.PCA(n_components=0.99, svd_solver= 'full')\n",
    "pca.fit(X_train)\n",
    "X_train_final = pca.transform(X_train)\n",
    "X_test_final = pca.transform(X_test)\n",
    "explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "print(explained_variance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_10fold = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "cv_5fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "clf_list = [KNeighborsClassifier(), DecisionTreeClassifier(), LogisticRegression(), RandomForestClassifier(), SVC(probability=True)]\n",
    "\n",
    "parameters_list = [\n",
    "        {\n",
    "                \"n_neighbors\": range(1,20,1),\n",
    "                \"weights\": [\"uniform\", \"distance\"],\n",
    "        },\n",
    "        {\n",
    "                \"criterion\": ['gini', 'entropy'],\n",
    "                \"min_samples_split\": list(range(2,40,2)),\n",
    "                \"max_features\": [1,2,3,4],\n",
    "                \"min_samples_leaf\": list(range(1,20,2)),\n",
    "        },\n",
    "        {\n",
    "                'solver': ['newton-cg', 'lbfgs', 'liblinear'],\n",
    "                'penalty' :[\"l1\", 'l2', 'elasticnet', 'none'],\n",
    "                'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "        },\n",
    "        {\n",
    "                \"n_estimators\" : range(20,100,5),\n",
    "                \"criterion\": ['gini', 'entropy'],\n",
    "                \"min_samples_split\": list(range(2,40,2)),\n",
    "                \"max_features\": [1,2,3,4],\n",
    "                \"min_samples_leaf\": list(range(1,20,2)),\n",
    "        },\n",
    "        {\n",
    "                'degree': [1, 3, 5],\n",
    "                'coef0': [0.01, 0.5, 1],\n",
    "                'C': [0.01, 0.5, 1],\n",
    "                'kernel': ['poly', 'rbf', 'sigmoid']\n",
    "        },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal n_neighbors=17\n",
      "The optimal weights=uniform\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAX9ElEQVR4nO3de7hddX3n8feHg0CQe4hUwiU4iSLeqEZBO1asokHrUGesgragtjJ4CeljO1Z9pqOOI9VpZyogiuggMFrRqlVUioKjMFoVAoRLBOwZrgkgIaANCRcTvvPHWpHNyT4nJ+Hs7JOs9+t59pO91m/ttb77sNmf9futvdZKVSFJ6q7thl2AJGm4DAJJ6jiDQJI6ziCQpI4zCCSp4wwCSeo4g0DTXpKlSY6Y5LK3JHn5OG1HJFk2lbVtriRnJ/lvA1z//Ume0j6fkeSbSX6V5B+SvCnJdx/n+l+R5Ovt8x2T3JDkSVNQuobAINCUGPsFnOSYJPcleUmSSvLtMct/PskHJ7PuqnpGVf1gaiserDROSnJdktVJlrVfws/aEtuvql2q6qZ28nXAPsDMqvrDqvpCVb3icW7iZOCj7bYeAs4C/vJxrlNDYhBoyiU5HjgdeDVwazv78CS/M7yqplaS7TeyyCnAIuAkYC/gqcDXaf4mW9qBwM+rau3jXVGSkSTPB3avqp/0NP09cHySHR/vNrTlGQSaUklOAP4H8Mqq+ueepv8OjDsUkuT3kyxJ8ssk/5zk2T1tv+lttMMc57S9jeuTvKfPcM+hSa5ph0K+lGSnMdt6f5J72vW+qWf+7knOTbIiya1J/nOS7dq2Nyf5UZK/S3Iv8MEkc5Nc0m7nniRfapedB7wTOLaq/k9VPVRVa9o98Y/2ee97JvlWu9372uf79bS/OclNSVYluXl9zeNtv22rtv1DwH8B3tAOF/1Ju74f9ix7cJKLktyb5MYkr+9pOzvJp5JckGQ18FLgKOCS3vdQVcuA+4DDx/tvrOnLINBUejvwYeBlVbV4TNvpwFP7jd8neS7N0MJ/BGYCnwbOH2fv8gPAHOApwJHAH/VZ5vXAAuAg4NnAm3vafgvYG5gNHA+cmeRpbdtpwO7tul8CHAe8pee1hwE3AU8CPtK+1+8CewL7ta8HeBmwrKou61NbP9sBn6PZcz8AeAD4BECSJwKnAkdV1a7Ai4Al7evG2/5vVNUHaIZxvtQOF/2v3vZ2/RfR7NE/CTgW+GSSZ/Qs9sb2/e4K/BB4FnBjn/dxPfCcSb5nTSMGgabSkcBPgGv7tD1I82XSr1fwNuDTVfXTqlpXVecAD9F/7/L1wMlVdV+7F3pqn2VOrao7qupe4JvAoWPa/6rdS78E+Dbw+iQjwBuA91XVqqq6haZn88c9r7ujqk6rqrVV9QDwa5ov732r6sGqWr+XPRO4s09dfVXVyqr6attrWEXzd3pJzyKPAM9MMqOq7qyqpe388ba/KX4fuKWqPte+ryuBr9IcV1jvG1X1o6p6pKoeBPYAVvVZ16q2TVsZg0BT6USasfDPJkmf9s8A+yR5zZj5BwJ/3g4L/TLJL4H9gX37rGNf4Pae6dv7LHNXz/M1wC490/dV1eqe6Vvbde4N7MCjxzTWt82eYFvvAQJcluaXTW9t568Entynrr6S7Jzk0+1w1L8ClwJ7JBlpa30Dzd/2ziTfTnLwRra/KQ4EDhvzt38TTc9pvbHv+z6a3sFYuwK/3IwaNGQGgabS3TTDIi8GPjm2sap+DXyIZkijNyhuBz5SVXv0PHauqi/22cadNMMg6+2/iTXu2Q6HrHcAcAdwD4/uYfe2Le99C2Pez11V9baq2pdmWOuTSeYC3wP2SzJ/kjX9OfA04LCq2g343XZ+2u18p6qOpAmXG2gCdaLtb4rbgUvG/O13qaq3j/e+gWtoAn+spwNXb+L2NQ0YBJpSVXUH8HvAgiR/12eR/w3sSDOGv95ngBOTHJbGE5O8Okm/vc4vA+9rD7DOBt61GWV+KMkOSV5MMzTyD1W1rl33R5LsmuRA4N3A58dbSZI/7Dmoex/NF+a6qvoXmiD8YppzF3ZIslOan9S+t8+qdqU5LvDLJHvRHAdZv419kvy7NrweAu4H1k20/U38W3yL5tjNHyd5Qvt4fpKnT/CaC3js0BXtf4u9aIYGtZUxCDTlqup2mjB4HfDXY9rW0XzR7dUzbzHNcYJP0HyhjfLYA7y9/iuwDLgZuBj4Cs0X5GTd1W7jDuALwIlVdUPbthBYTXNA+Ic0B1DPmmBdzwd+muR+4HxgUVXd3Lad1L6f02mGS/4f8FqaYxZjfRyYQdMr+QlwYU/bdjQ9hjuAe2m+gN8xie1PSntM4hXAMe027gI+RhPW473mSuBXSQ7rmf1G4Jz2nAJtZeKNabQ1S/J24JiqeslGF9aUSfIK4B1V9Qftr7uuBn63qu4ecmnaDAaBtipJnkzz884fA/NofvXziar6+DDrkrZmGzs7UppudqA5z+AgmiGX8+hzYFrS5NkjkKSO82CxJHXcVjc0tPfee9ecOXOGXYYkbVWuuOKKe6pqVr+2rS4I5syZw+LFYy9jI0maSJJbx2tzaEiSOs4gkKSOMwgkqeMMAknqOINAkjpuYEGQ5Kwkdye5bpz2JDk1yWia2wo+d1C1SJLGN8gewdk89lLDYx1Fc62YecAJwKcGWIskaRwDO4+gqi5NMmeCRY4Gzq3mGhc/SbJHkidX1aRv8bc5TjvtNEZHRwe5iY1avnw5DzzwwFBrmE5mzJjB7NmzN77ggM2dO5eFCxcOuwxpixvmCWWzeewt8Ja18zYIgiQn0PQaOOCAAx7XRkdHR1ly3fWs23mvjS88INs9uIY88uuhbX+6WfVwcddDvxhqDSNr7h3q9qVhGmYQ9Lunbd8r4FXVmcCZAPPnz3/cV8lbt/NePHDwqx7varQNmXHDBcMuQRqaYf5qaBmPvd/sfjR3SJIkbUHDDILzgePaXw8dDvxq0McHJEkbGtjQUJIvAkcAeydZRnOf2icAVNUZNDfAfhXN/WnXAG8ZVC2SpPEN8ldDx26kvYB3Dmr7kqTJ8cxiSeo4g0CSOs4gkKSOMwgkqeMMAknqOINAkjrOIJCkjjMIJKnjDAJJ6jiDQJI6ziCQpI4b5v0IJE1D0+UufoB3rttCDAJJ0463ct2yDAJJjzEd9n4XLVoEwCmnnDLkSrrBYwSS1HEGgSR1nEEgSR1nEEhSxxkEktRxBoEkdZxBIEkd17nzCJYvX87Iml8x44YLhl2KppGRNStZvnztsMuQhsIegSR1XOd6BLNnz+auh7bngYNfNexSNI3MuOECZs/eZ9hlSENhj0CSOs4gkKSOMwgkqeMMAknqOINAkjrOIJCkjjMIJKnjDAJJ6jiDQJI6ziCQpI4zCCSp4wwCSeo4g0CSOs4gkKSOG2gQJFmQ5MYko0ne26d9zyT/mOSaJJcleeYg65EkbWhgQZBkBDgdOAo4BDg2ySFjFns/sKSqng0cB5wyqHokSf0NskfwAmC0qm6qqoeB84CjxyxzCPA9gKq6AZiTxLuDSNIWNMggmA3c3jO9rJ3X62rg3wMkeQFwILDf2BUlOSHJ4iSLV6xYMaByJambBhkE6TOvxkx/FNgzyRJgIXAVsMEdxKvqzKqaX1XzZ82aNeWFSlKXDfKexcuA/Xum9wPu6F2gqv4VeAtAkgA3tw+pc0477TRGR0eHXca0sP7vsGjRoiFXMj3MnTuXhQsXDmz9gwyCy4F5SQ4ClgPHAG/sXSDJHsCa9hjCnwKXtuEgdc7o6Cj/svQqDthl3bBLGbodft0MVjx06+IhVzJ8t90/MvBtDCwIqmptkncB3wFGgLOqammSE9v2M4CnA+cmWQf8DPiTQdUjbQ0O2GUd73+u+0J61MlX7jbwbQyyR0BVXQBcMGbeGT3PfwzMG2QNkqSJeWaxJHWcQSBJHWcQSFLHDfQYwXQ1suZeZtxwwcYX3MZt92BzUPKRnQZ/MGq6G1lzL+BJ7eqmzgXB3Llzh13CtDE6ugqAuU/xCxD28bOhzupcEAzypIytzfqTdU45xWv9SV3WuSCYDqbLGaTT5ezNQZ81KWliBkGHzZgxY9glSJoGDIIhcO9X0nTiz0clqeMMAknqOINAkjrOIJCkjjMIJKnjDAJJ6jiDQJI6zvMIpGli+fLlrF41skXuSKWtx62rRnji8uUD3YY9AknqOHsE0jQxe/ZsHlp7p/cs1mOcfOVu7Dh79kC3YY9AkjrOIJCkjjMIJKnjDAJJ6jiDQJI6ziCQpI4zCCSp4wwCSeo4g0CSOs4gkKSOMwgkqeMMAknqOINAkjrOIJCkjjMIJKnjDAJJ6jiDQJI6ziCQpI4baBAkWZDkxiSjSd7bp333JN9McnWSpUneMsh6JEkbGtg9i5OMAKcDRwLLgMuTnF9VP+tZ7J3Az6rqNUlmATcm+UJVPTyouqTp7Lb7Rzj5yt2GXcbQ/WJNs4+6z86PDLmS4bvt/hHmDXgbg7x5/QuA0aq6CSDJecDRQG8QFLBrkgC7APcCawdYkzRtzZ07d9glTBsPj44CsOOB/k3mMfjPxqSCIMnhwNKqWtVO7wocUlU/neBls4Hbe6aXAYeNWeYTwPnAHcCuwBuqaoNdgCQnACcAHHDAAZMpWdrqLFy4cNglTBuLFi0C4JRTThlyJd0w2WMEnwLu75le3c6bSPrMqzHTrwSWAPsChwKfSLJBv7iqzqyq+VU1f9asWZMsWZI0GZMNglTVb77E2732jfUmlgH790zvR7Pn3+stwNeqMQrcDBw8yZokSVNgskFwU5KTkjyhfSwCbtrIay4H5iU5KMkOwDE0w0C9bgNeBpBkH+Bpk1ivJGkKTTYITgReBCzn0bH+EyZ6QVWtBd4FfAe4HvhyVS1NcmKSE9vFPgy8KMm1wPeAv6yqezb9bUiSNtekDhZX1d00e/SbpKouAC4YM++Mnud3AK/Y1PVKkqbOZH819Dk2PNBLVb11yiuSJG1Rkz2P4Fs9z3cCXsuGB34lSVuhyQ4NfbV3OskXgYsHUpEkaYva3GsNzQM8s0uStgGTPUawikePERTwC+A9gypKkrTlTHZoaNcke9H0BHZaP3tgVUmStpjJ9gj+FFhEc3bwEuBw4MfA7w2sMknSFjHZYwSLgOcDt1bVS4HfBlYMrCpJ0hYz2SB4sKoeBEiyY1XdQHM5CEnSVm6y5xEsS7IH8HXgoiT34XkEkrRNmOzB4te2Tz+Y5PvA7sCFA6tKkrTFbPIdyqrqkkEUIkkajoHevF6SNP0ZBJLUcQaBJHWcQSBJHWcQSFLHGQSS1HEGgSR1nEEgSR1nEEhSxxkEktRxBoEkdZxBIEkdZxBIUscZBJLUcQaBJHWcQSBJHbfJN6aRtG077bTTGB0dHWoN67e/aNGiodYBMHfuXBYuXDjsMgbKIJA07cyYMWPYJXSKQSDpMbb1vV9tyGMEktRxBoEkdZxBIEkdZxBIUscZBJLUcQaBJHWcQSBJHTfQIEiyIMmNSUaTvLdP+39KsqR9XJdkXZK9BlmTJOmxBhYESUaA04GjgEOAY5Mc0rtMVf1NVR1aVYcC7wMuqap7B1WTJGlDg+wRvAAYraqbquph4Dzg6AmWPxb44gDrkST1McggmA3c3jO9rJ23gSQ7AwuAr47TfkKSxUkWr1ixYsoLlaQuG2QQpM+8GmfZ1wA/Gm9YqKrOrKr5VTV/1qxZU1agJGmwQbAM2L9nej/gjnGWPQaHhSRpKAYZBJcD85IclGQHmi/788culGR34CXANwZYiyRpHAO7DHVVrU3yLuA7wAhwVlUtTXJi235Gu+hrge9W1epB1SJJGl+qxhu2n57mz59fixcvHnYZkrRVSXJFVc3v1+aZxZLUcQaBJHWcQSBJHWcQSFLHGQSS1HEGgSR1nEEgSR1nEEhSxxkEktRxBoEkdZxBIEkdZxBIUscZBJLUcQaBJHWcQSBJHWcQSFLHGQSS1HEGgSR1nEEgSR1nEEhSxxkEktRxBoEkdZxBIEkdZxBIUscZBJLUcQaBJHWcQSBJHWcQSFLHGQSS1HEGgSR1nEEgSR1nEEhSxxkEktRxBoEkdZxBIEkdZxBIUscZBJLUcQMNgiQLktyYZDTJe8dZ5ogkS5IsTXLJIOuRJG1o+0GtOMkIcDpwJLAMuDzJ+VX1s55l9gA+CSyoqtuSPGlQ9UiS+htkj+AFwGhV3VRVDwPnAUePWeaNwNeq6jaAqrp7gPVIkvoYZBDMBm7vmV7Wzuv1VGDPJD9IckWS4/qtKMkJSRYnWbxixYoBlStJ3TTIIEifeTVmenvgecCrgVcCf5XkqRu8qOrMqppfVfNnzZo19ZVKUocN7BgBTQ9g/57p/YA7+ixzT1WtBlYnuRR4DvDzAdYlSeoxyB7B5cC8JAcl2QE4Bjh/zDLfAF6cZPskOwOHAdcPsCZJ0hgD6xFU1dok7wK+A4wAZ1XV0iQntu1nVNX1SS4ErgEeAT5bVdcNqiZJ0oZSNXbYfnqbP39+LV68eNhlSNJWJckVVTW/X5tnFktSxxkEktRxBoGkaWflypWcdNJJrFy5ctildIJBIGnaOeecc7j22ms599xzh11KJxgEkqaVlStXcuGFF1JVXHjhhfYKtgCDQNK0cs455/DII48AsG7dOnsFW4BBIGlaufjii1m7di0Aa9eu5aKLLhpyRds+g0DStPLyl7+c7bdvznXdfvvtOfLII4dc0bbPIJA0rRx//PFst13z1TQyMsJxx/W9KLGmkEEgaVqZOXMmCxYsIAkLFixg5syZwy5pmzfIq49K0mY5/vjjueWWW+wNbCEGgaRpZ+bMmZx66qnDLqMzHBqSpI4zCCSp4wwCSeo4g0CSOm6ruzFNkhXArcOuYxuyN3DPsIuQ+vCzObUOrKpZ/Rq2uiDQ1EqyeLy7FknD5Gdzy3FoSJI6ziCQpI4zCHTmsAuQxuFncwvxGIEkdZw9AknqOINAkjrOINjGJNkjyTs243UXJNljACVJACS5v/133yRfGWeZHySZ8CejSf4syc490352HyePEWxjkswBvlVVzxwzf6Sq1g2nKqkJgqraZSPL/AD4i6paPMEytwDzq8qTzaaIPYJtz0eBf5NkSZLLk3w/yd8D1wIk+XqSK5IsTXLC+hcluSXJ3knmJLk+yWfaZb6bZMaw3oymryQf6+19Jvlgkg8k+V6SK5Ncm+ToPq+bk+S69vmMJOcluSbJl4AZPct9Ksni9nP4oXbeScC+wPeTfL+dd0uSvdvn705yXfv4s57t+ZmeSFX52IYewBzguvb5EcBq4KCe9r3af2cA1wEz2+lbaE7pnwOsBQ5t538Z+KNhvy8f0+8B/DZwSc/0z4ADgN3a6b2BUR4debi//bf3M/pu4Kz2+bPbz978dnr9Z3UE+AHw7Hb6FmDvnu2u/+w+j2aH54nALsDStkY/0xt52CPY9l1WVTf3TJ+U5GrgJ8D+wLw+r7m5qpa0z6+g+R9Jeoyqugp4Ujvm/xzgPuBO4OQk1wAXA7OBfSZYze8Cn2/Xdw1wTU/b65NcCVwFPAM4ZCMl/VvgH6tqdVXdD3wNeHHb5md6At6hbNu3ev2TJEcALwdeWFVr2vHYnfq85qGe5+vo6a5LY3wFeB3wW8B5wJuAWcDzqurX7Xh+v89Yrw0OVCY5CPgL4PlVdV+SsyexnkzQ5md6AvYItj2rgF3HadsduK8NgYOBw7dcWdpGnQccQxMGX6H5jN3dhsBLgQM38vpLacKDJM+kGR4C2I1mJ+ZXSfYBjup5zXif8UuBP0iyc5InAq8F/u9mvauOsUewjamqlUl+1B6MewD4RU/zhcCJbbf9RprhIWmzVdXSJLsCy6vqziRfAL6ZZDGwBLhhI6v4FPC59jO5BLisXe/VSa6iGee/CfhRz2vOBP4pyZ1V9dKeWq5sew6XtbM+W1VXtb+k0wT8+agkdZxDQ5LUcQaBJHWcQSBJHWcQSFLHGQSS1HEGgTRASd6cZN9h1yFNxCCQBuvNNBdJk6YtzyOQNlF71uqXgf1oLoj2YZqLq/1Pmoud3UMTAL8DnA0spzm574VV9cCWr1iamEEgbaIk/wFYUFVva6d3B/4JOLqqViR5A/DKqnrrZK6vLw2bl5iQNt21wN8m+RjwLZqrbj4TuCgJNL2EO4dXnrRpDAJpE1XVz5M8D3gV8NfARcDSqnrhcCuTNo8Hi6VN1P4KaE1VfR74W+AwYFaSF7btT0jyjHbxia4GK00L9gikTfcs4G+SPAL8Gng7zR2wTm2PF2wPfJzmyplnA2ck8WCxpi0PFktSxzk0JEkdZxBIUscZBJLUcQaBJHWcQSBJHWcQSFLHGQSS1HH/HyXzl1al3JiNAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = []\n",
    "for train_index, validation_index, in cv_10fold.split(X_train, Y_train):\n",
    "    \n",
    "    X_train_CV = X_train[train_index]\n",
    "    Y_train_CV = Y_train[train_index]\n",
    "\n",
    "    X_validation_CV = X_train[validation_index]\n",
    "    Y_validation_CV = Y_train[validation_index]\n",
    "    \n",
    "    clf = clf_list[0]\n",
    "    parameters = parameters_list[0]\n",
    "    randomized_search = RandomizedSearchCV(clf, parameters, cv=cv_5fold, scoring='roc_auc', n_iter = 10)\n",
    "    randomized_search.fit(X_train_CV, Y_train_CV)\n",
    "    \n",
    "    # Get resulting classifier\n",
    "    clf = randomized_search.best_estimator_\n",
    "\n",
    "    # Test the classifier on the train data\n",
    "    probabilities = clf.predict_proba(X_train_CV)\n",
    "    scores = probabilities[:, 1]\n",
    "    \n",
    "    # Get the auc\n",
    "    auc = metrics.roc_auc_score(Y_train_CV, scores)\n",
    "    results.append({\n",
    "        'auc': auc,\n",
    "        'n_neighbors': clf.n_neighbors,\n",
    "        \"weights\": clf.weights,\n",
    "        \"metric\": clf.metric,\n",
    "        'set': 'train'\n",
    "    })\n",
    "    \n",
    "    # Test the classifier on the validation data\n",
    "    probabilities_validation = clf.predict_proba(X_validation_CV)\n",
    "    scores_validation = probabilities_validation[:, 1]\n",
    "    \n",
    "    # Get the auc\n",
    "    auc_validation = metrics.roc_auc_score(Y_validation_CV, scores_validation)\n",
    "    results.append({\n",
    "        'auc': auc_validation,\n",
    "        'n_neighbors': clf.n_neighbors,\n",
    "        \"weights\": clf.weights,\n",
    "        \"metric\": clf.metric,\n",
    "        'set': 'validation'\n",
    "    })\n",
    "    \n",
    "#Create results dataframe and plot it\n",
    "results = pd.DataFrame(results)\n",
    "sns.boxplot(y='auc', x='set', data=results).set_title(f'{clf_list[0]}')\n",
    "\n",
    "optimal_parameter = []\n",
    "parameter_keys = list(parameters.keys())\n",
    "\n",
    "for item in parameter_keys:\n",
    "    best_item = [] \n",
    "    for i in list(range(0,10,2)):\n",
    "        best_item.append(results[item][i])\n",
    "    \n",
    "    optimal_parameter.append(statistics.median(best_item))\n",
    "    print(f\"The optimal {item}={optimal_parameter[-1]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal criterion=gini\n",
      "The optimal min_samples_split=12\n",
      "The optimal max_features=4\n",
      "The optimal min_samples_leaf=5\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = []\n",
    "for train_index, validation_index, in cv_10fold.split(X_train, Y_train):\n",
    "    \n",
    "    X_train_CV = X_train[train_index]\n",
    "    Y_train_CV = Y_train[train_index]\n",
    "\n",
    "    X_validation_CV = X_train[validation_index]\n",
    "    Y_validation_CV = Y_train[validation_index]\n",
    "    \n",
    "    clf = clf_list[1]\n",
    "    parameters = parameters_list[1]\n",
    "    randomized_search = RandomizedSearchCV(clf, parameters, cv=cv_5fold, scoring='roc_auc', n_iter = 10)\n",
    "    randomized_search.fit(X_train_CV, Y_train_CV)\n",
    "    \n",
    "    # Get resulting classifier\n",
    "    clf = randomized_search.best_estimator_\n",
    "\n",
    "    # Test the classifier on the train data\n",
    "    probabilities = clf.predict_proba(X_train_CV)\n",
    "    scores = probabilities[:, 1]\n",
    "    \n",
    "    # Get the auc\n",
    "    auc = metrics.roc_auc_score(Y_train_CV, scores)\n",
    "    results.append({\n",
    "        'auc': auc_validation,\n",
    "        'criterion': clf.criterion,\n",
    "        'min_samples_split': clf.min_samples_split,\n",
    "        'max_features': clf.max_features,\n",
    "        \"min_samples_leaf\": clf.min_samples_leaf,\n",
    "        'set': 'train'\n",
    "    })\n",
    "    \n",
    "    # Test the classifier on the validation data\n",
    "    probabilities_validation = clf.predict_proba(X_validation_CV)\n",
    "    scores_validation = probabilities_validation[:, 1]\n",
    "    \n",
    "    # Get the auc\n",
    "    auc_validation = metrics.roc_auc_score(Y_validation_CV, scores_validation)\n",
    "    results.append({\n",
    "        'auc': auc_validation,\n",
    "        'criterion': clf.criterion,\n",
    "        'min_samples_split': clf.min_samples_split,\n",
    "        'max_features': clf.max_features,\n",
    "        \"min_samples_leaf\": clf.min_samples_leaf,\n",
    "        'set': 'validation'\n",
    "    })\n",
    "    \n",
    "#Create results dataframe and plot it\n",
    "results = pd.DataFrame(results)\n",
    "sns.boxplot(y='auc', x='set', data=results).set_title(f'{clf_list[1]}')\n",
    "\n",
    "optimal_parameter = []\n",
    "parameter_keys = list(parameters.keys())\n",
    "\n",
    "for item in parameter_keys:\n",
    "    best_item = [] \n",
    "    for i in list(range(0,10,2)):\n",
    "        best_item.append(results[item][i])\n",
    "    \n",
    "    optimal_parameter.append(statistics.median(best_item))\n",
    "    print(f\"The optimal {item}={optimal_parameter[-1]}\")\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "10 fits failed out of a total of 50.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 464, in _check_solver\n",
      "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan 0.8637037  0.86534392        nan 0.8637037  0.88878307\n",
      " 0.87010582 0.85174603 0.85777778 0.84820106]\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "30 fits failed out of a total of 50.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 457, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan 0.86883598        nan 0.88756614\n",
      " 0.88645503        nan        nan 0.88645503]\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "30 fits failed out of a total of 50.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 464, in _check_solver\n",
      "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 457, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan 0.87619048        nan        nan 0.85767196        nan\n",
      " 0.83142857        nan 0.87619048        nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "10 fits failed out of a total of 50.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 457, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.85402116 0.66592593        nan 0.86888889 0.85185185 0.56441799\n",
      " 0.89132275 0.88989418        nan 0.88666667]\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "30 fits failed out of a total of 50.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 464, in _check_solver\n",
      "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan 0.85941799        nan        nan        nan 0.81301587\n",
      "        nan        nan 0.85925926 0.85941799]\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "10 fits failed out of a total of 50.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 464, in _check_solver\n",
      "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.88253968        nan 0.84867725 0.86777778 0.89275132 0.89275132\n",
      " 0.88550265 0.70925926        nan 0.86380952]\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "25 fits failed out of a total of 50.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 457, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan 0.88497354 0.88761905\n",
      "        nan 0.58759259 0.88936508 0.88185185]\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "20 fits failed out of a total of 50.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 457, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.89804233 0.93037037        nan 0.89809524        nan 0.89079365\n",
      "        nan 0.87756614        nan 0.86920635]\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "35 fits failed out of a total of 50.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 457, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan 0.87814815        nan        nan 0.8737037         nan\n",
      "        nan 0.91767196        nan        nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "15 fits failed out of a total of 50.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 457, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan 0.83222222 0.86148148 0.86296296 0.87148148 0.78820106\n",
      " 0.83259259        nan        nan 0.87148148]\n",
      "  warnings.warn(\n",
      "C:\\Users\\judit\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'LogisticRegression()')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = []\n",
    "for train_index, validation_index, in cv_10fold.split(X_train, Y_train):\n",
    "    \n",
    "    X_train_CV = X_train[train_index]\n",
    "    Y_train_CV = Y_train[train_index]\n",
    "\n",
    "    X_validation_CV = X_train[validation_index]\n",
    "    Y_validation_CV = Y_train[validation_index]\n",
    "    \n",
    "    clf = clf_list[2]\n",
    "    parameters = parameters_list[2]\n",
    "    randomized_search = RandomizedSearchCV(clf, parameters, cv=cv_5fold, scoring='roc_auc', n_iter = 10)\n",
    "    randomized_search.fit(X_train_CV, Y_train_CV)\n",
    "    \n",
    "    # Get resulting classifier\n",
    "    clf = randomized_search.best_estimator_\n",
    "\n",
    "    # Test the classifier on the train data\n",
    "    probabilities = clf.predict_proba(X_train_CV)\n",
    "    scores = probabilities[:, 1]\n",
    "    \n",
    "    # Get the auc\n",
    "    auc = metrics.roc_auc_score(Y_train_CV, scores)\n",
    "    results.append({\n",
    "        'auc': auc,\n",
    "        'solver' : clf.solver,\n",
    "        'penalty' : clf.penalty,\n",
    "        'C' : clf.C,\n",
    "        'set': 'test',\n",
    "    })\n",
    "    \n",
    "    # Test the classifier on the validation data\n",
    "    probabilities_validation = clf.predict_proba(X_validation_CV)\n",
    "    scores_validation = probabilities_validation[:, 1]\n",
    "    \n",
    "    # Get the auc\n",
    "    auc_validation = metrics.roc_auc_score(Y_validation_CV, scores_validation)\n",
    "    results.append({\n",
    "        'auc': auc_validation,\n",
    "        'solver' : clf.solver,\n",
    "        'C' : clf.C,\n",
    "        'penalty' : clf.penalty,\n",
    "        'set': 'validation'\n",
    "    })\n",
    "    \n",
    "#Create results dataframe and plot it\n",
    "results = pd.DataFrame(results)\n",
    "sns.boxplot(y='auc', x='set', data=results).set_title(f'{clf_list[2]}')\n",
    "\n",
    "# optimal_parameter = []\n",
    "# parameter_keys = list(parameters.keys())\n",
    "\n",
    "# for item in parameter_keys:\n",
    "#     best_item = [] \n",
    "#     for i in list(range(0,10,2)):\n",
    "#         best_item.append(results[item][i])\n",
    "    \n",
    "#     optimal_parameter.append(statistics.median(best_item))\n",
    "#     print(f\"The optimal {item}={optimal_parameter[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal n_estimators=nan\n",
      "The optimal criterion=gini\n",
      "The optimal min_samples_split=24\n",
      "The optimal max_features=4\n",
      "The optimal min_samples_leaf=3\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = []\n",
    "for train_index, validation_index, in cv_10fold.split(X_train, Y_train):\n",
    "    \n",
    "    X_train_CV = X_train[train_index]\n",
    "    Y_train_CV = Y_train[train_index]\n",
    "\n",
    "    X_validation_CV = X_train[validation_index]\n",
    "    Y_validation_CV = Y_train[validation_index]\n",
    "    \n",
    "    clf = clf_list[3]\n",
    "    parameters = parameters_list[3]\n",
    "    randomized_search = RandomizedSearchCV(clf, parameters, cv=cv_5fold, scoring='roc_auc', n_iter = 10)\n",
    "    randomized_search.fit(X_train_CV, Y_train_CV)\n",
    "    \n",
    "    # Get resulting classifier\n",
    "    clf = randomized_search.best_estimator_\n",
    "\n",
    "    # Test the classifier on the train data\n",
    "    probabilities = clf.predict_proba(X_train_CV)\n",
    "    scores = probabilities[:, 1]\n",
    "    \n",
    "    # Get the auc\n",
    "    auc = metrics.roc_auc_score(Y_train_CV, scores)\n",
    "    results.append({\n",
    "        'auc': auc,\n",
    "        \"criterion\": clf.criterion,\n",
    "        \"min_samples_split\": clf.min_samples_split,\n",
    "        \"max_features\": clf.max_features,\n",
    "        \"min_samples_leaf\": clf.min_samples_leaf,\n",
    "        \"n_estimators\" : clf.n_estimators,\n",
    "        'set': 'test'\n",
    "    })\n",
    "    \n",
    "    # Test the classifier on the validation data\n",
    "    probabilities_validation = clf.predict_proba(X_validation_CV)\n",
    "    scores_validation = probabilities_validation[:, 1]\n",
    "    \n",
    "    # Get the auc\n",
    "    auc_validation = metrics.roc_auc_score(Y_validation_CV, scores_validation)\n",
    "    results.append({\n",
    "        'auc': auc_validation,\n",
    "        \"criterion\": clf.criterion,\n",
    "        \"min_samples_split\": clf.min_samples_split,\n",
    "        \"max_features\": clf.max_features,\n",
    "        \"min_samples_leaf\": clf.min_samples_leaf,\n",
    "        \"n_estimators\" : clf.n_estimators,\n",
    "        'set': 'validation'\n",
    "\n",
    "    })\n",
    "\n",
    "    \n",
    "    \n",
    "#Create results dataframe and plot it\n",
    "results = pd.DataFrame(results)\n",
    "sns.boxplot(y='auc', x='set', data=results).set_title(f'{clf_list[3]}')\n",
    "\n",
    "optimal_parameter = []\n",
    "parameter_keys = list(parameters.keys())\n",
    "\n",
    "for item in parameter_keys:\n",
    "    best_item = [] \n",
    "    for i in list(range(0,10,2)):\n",
    "        best_item.append(results[item][i])\n",
    "    \n",
    "    optimal_parameter.append(statistics.median(best_item))\n",
    "    print(f\"The optimal {item}={optimal_parameter[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV 1/5] END C=1, coef0=1, degree=3, kernel=sigmoid;, score=0.793 total time=   0.0s\n",
      "[CV 2/5] END C=1, coef0=1, degree=3, kernel=sigmoid;, score=0.859 total time=   0.0s\n",
      "[CV 3/5] END C=1, coef0=1, degree=3, kernel=sigmoid;, score=0.733 total time=   0.0s\n",
      "[CV 4/5] END C=1, coef0=1, degree=3, kernel=sigmoid;, score=0.326 total time=   0.0s\n",
      "[CV 5/5] END C=1, coef0=1, degree=3, kernel=sigmoid;, score=0.794 total time=   0.0s\n",
      "[CV 1/5] END C=0.5, coef0=0.01, degree=1, kernel=sigmoid;, score=0.807 total time=   0.0s\n",
      "[CV 2/5] END C=0.5, coef0=0.01, degree=1, kernel=sigmoid;, score=0.867 total time=   0.0s\n",
      "[CV 3/5] END C=0.5, coef0=0.01, degree=1, kernel=sigmoid;, score=0.741 total time=   0.0s\n",
      "[CV 4/5] END C=0.5, coef0=0.01, degree=1, kernel=sigmoid;, score=0.296 total time=   0.0s\n",
      "[CV 5/5] END C=0.5, coef0=0.01, degree=1, kernel=sigmoid;, score=0.778 total time=   0.0s\n",
      "[CV 1/5] END C=0.01, coef0=1, degree=3, kernel=rbf;, score=0.900 total time=   0.0s\n",
      "[CV 2/5] END C=0.01, coef0=1, degree=3, kernel=rbf;, score=0.896 total time=   0.0s\n",
      "[CV 3/5] END C=0.01, coef0=1, degree=3, kernel=rbf;, score=0.600 total time=   0.0s\n",
      "[CV 4/5] END C=0.01, coef0=1, degree=3, kernel=rbf;, score=0.689 total time=   0.0s\n",
      "[CV 5/5] END C=0.01, coef0=1, degree=3, kernel=rbf;, score=0.484 total time=   0.0s\n",
      "[CV 1/5] END C=0.5, coef0=1, degree=1, kernel=sigmoid;, score=0.793 total time=   0.0s\n",
      "[CV 2/5] END C=0.5, coef0=1, degree=1, kernel=sigmoid;, score=0.859 total time=   0.0s\n",
      "[CV 3/5] END C=0.5, coef0=1, degree=1, kernel=sigmoid;, score=0.733 total time=   0.0s\n",
      "[CV 4/5] END C=0.5, coef0=1, degree=1, kernel=sigmoid;, score=0.326 total time=   0.0s\n",
      "[CV 5/5] END C=0.5, coef0=1, degree=1, kernel=sigmoid;, score=0.794 total time=   0.0s\n",
      "[CV 1/5] END C=0.5, coef0=0.5, degree=3, kernel=sigmoid;, score=0.793 total time=   0.0s\n",
      "[CV 2/5] END C=0.5, coef0=0.5, degree=3, kernel=sigmoid;, score=0.859 total time=   0.0s\n",
      "[CV 3/5] END C=0.5, coef0=0.5, degree=3, kernel=sigmoid;, score=0.733 total time=   0.0s\n",
      "[CV 4/5] END C=0.5, coef0=0.5, degree=3, kernel=sigmoid;, score=0.311 total time=   0.0s\n",
      "[CV 5/5] END C=0.5, coef0=0.5, degree=3, kernel=sigmoid;, score=0.794 total time=   0.0s\n",
      "[CV 1/5] END C=1, coef0=1, degree=5, kernel=rbf;, score=0.950 total time=   0.0s\n",
      "[CV 2/5] END C=1, coef0=1, degree=5, kernel=rbf;, score=0.881 total time=   0.0s\n",
      "[CV 3/5] END C=1, coef0=1, degree=5, kernel=rbf;, score=0.593 total time=   0.0s\n",
      "[CV 4/5] END C=1, coef0=1, degree=5, kernel=rbf;, score=0.689 total time=   0.0s\n",
      "[CV 5/5] END C=1, coef0=1, degree=5, kernel=rbf;, score=0.548 total time=   0.0s\n",
      "[CV 1/5] END C=0.01, coef0=1, degree=5, kernel=rbf;, score=0.900 total time=   0.0s\n",
      "[CV 2/5] END C=0.01, coef0=1, degree=5, kernel=rbf;, score=0.896 total time=   0.0s\n",
      "[CV 3/5] END C=0.01, coef0=1, degree=5, kernel=rbf;, score=0.600 total time=   0.0s\n",
      "[CV 4/5] END C=0.01, coef0=1, degree=5, kernel=rbf;, score=0.689 total time=   0.0s\n",
      "[CV 5/5] END C=0.01, coef0=1, degree=5, kernel=rbf;, score=0.484 total time=   0.0s\n",
      "[CV 1/5] END C=0.5, coef0=1, degree=5, kernel=sigmoid;, score=0.793 total time=   0.0s\n",
      "[CV 2/5] END C=0.5, coef0=1, degree=5, kernel=sigmoid;, score=0.859 total time=   0.0s\n",
      "[CV 3/5] END C=0.5, coef0=1, degree=5, kernel=sigmoid;, score=0.733 total time=   0.0s\n",
      "[CV 4/5] END C=0.5, coef0=1, degree=5, kernel=sigmoid;, score=0.326 total time=   0.0s\n",
      "[CV 5/5] END C=0.5, coef0=1, degree=5, kernel=sigmoid;, score=0.794 total time=   0.0s\n",
      "[CV 1/5] END C=0.01, coef0=0.5, degree=1, kernel=sigmoid;, score=0.786 total time=   0.0s\n",
      "[CV 2/5] END C=0.01, coef0=0.5, degree=1, kernel=sigmoid;, score=0.889 total time=   0.0s\n",
      "[CV 3/5] END C=0.01, coef0=0.5, degree=1, kernel=sigmoid;, score=0.741 total time=   0.0s\n",
      "[CV 4/5] END C=0.01, coef0=0.5, degree=1, kernel=sigmoid;, score=0.311 total time=   0.0s\n",
      "[CV 5/5] END C=0.01, coef0=0.5, degree=1, kernel=sigmoid;, score=0.794 total time=   0.0s\n",
      "[CV 1/5] END C=0.01, coef0=1, degree=3, kernel=sigmoid;, score=0.786 total time=   0.0s\n",
      "[CV 2/5] END C=0.01, coef0=1, degree=3, kernel=sigmoid;, score=0.889 total time=   0.0s\n",
      "[CV 3/5] END C=0.01, coef0=1, degree=3, kernel=sigmoid;, score=0.741 total time=   0.0s\n",
      "[CV 4/5] END C=0.01, coef0=1, degree=3, kernel=sigmoid;, score=0.326 total time=   0.0s\n",
      "[CV 5/5] END C=0.01, coef0=1, degree=3, kernel=sigmoid;, score=0.667 total time=   0.0s\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV 1/5] END C=1, coef0=1, degree=3, kernel=sigmoid;, score=0.786 total time=   0.0s\n",
      "[CV 2/5] END C=1, coef0=1, degree=3, kernel=sigmoid;, score=0.815 total time=   0.0s\n",
      "[CV 3/5] END C=1, coef0=1, degree=3, kernel=sigmoid;, score=0.778 total time=   0.0s\n",
      "[CV 4/5] END C=1, coef0=1, degree=3, kernel=sigmoid;, score=0.844 total time=   0.0s\n",
      "[CV 5/5] END C=1, coef0=1, degree=3, kernel=sigmoid;, score=0.476 total time=   0.0s\n",
      "[CV 1/5] END C=1, coef0=0.5, degree=5, kernel=sigmoid;, score=0.786 total time=   0.0s\n",
      "[CV 2/5] END C=1, coef0=0.5, degree=5, kernel=sigmoid;, score=0.778 total time=   0.0s\n",
      "[CV 3/5] END C=1, coef0=0.5, degree=5, kernel=sigmoid;, score=0.778 total time=   0.0s\n",
      "[CV 4/5] END C=1, coef0=0.5, degree=5, kernel=sigmoid;, score=0.830 total time=   0.0s\n",
      "[CV 5/5] END C=1, coef0=0.5, degree=5, kernel=sigmoid;, score=0.484 total time=   0.0s\n",
      "[CV 1/5] END C=0.5, coef0=1, degree=5, kernel=poly;, score=0.886 total time=   0.0s\n",
      "[CV 2/5] END C=0.5, coef0=1, degree=5, kernel=poly;, score=0.711 total time=   0.0s\n",
      "[CV 3/5] END C=0.5, coef0=1, degree=5, kernel=poly;, score=0.904 total time=   0.0s\n",
      "[CV 4/5] END C=0.5, coef0=1, degree=5, kernel=poly;, score=0.874 total time=   0.0s\n",
      "[CV 5/5] END C=0.5, coef0=1, degree=5, kernel=poly;, score=0.730 total time=   0.0s\n",
      "[CV 1/5] END C=0.01, coef0=0.5, degree=1, kernel=rbf;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END C=0.01, coef0=0.5, degree=1, kernel=rbf;, score=0.815 total time=   0.0s\n",
      "[CV 3/5] END C=0.01, coef0=0.5, degree=1, kernel=rbf;, score=0.889 total time=   0.0s\n",
      "[CV 4/5] END C=0.01, coef0=0.5, degree=1, kernel=rbf;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END C=0.01, coef0=0.5, degree=1, kernel=rbf;, score=0.794 total time=   0.0s\n",
      "[CV 1/5] END C=1, coef0=0.01, degree=5, kernel=sigmoid;, score=0.771 total time=   0.0s\n",
      "[CV 2/5] END C=1, coef0=0.01, degree=5, kernel=sigmoid;, score=0.756 total time=   0.0s\n",
      "[CV 3/5] END C=1, coef0=0.01, degree=5, kernel=sigmoid;, score=0.778 total time=   0.0s\n",
      "[CV 4/5] END C=1, coef0=0.01, degree=5, kernel=sigmoid;, score=0.822 total time=   0.0s\n",
      "[CV 5/5] END C=1, coef0=0.01, degree=5, kernel=sigmoid;, score=0.500 total time=   0.0s\n",
      "[CV 1/5] END C=1, coef0=0.5, degree=3, kernel=poly;, score=0.843 total time=   0.0s\n",
      "[CV 2/5] END C=1, coef0=0.5, degree=3, kernel=poly;, score=0.696 total time=   0.0s\n",
      "[CV 3/5] END C=1, coef0=0.5, degree=3, kernel=poly;, score=0.852 total time=   0.0s\n",
      "[CV 4/5] END C=1, coef0=0.5, degree=3, kernel=poly;, score=0.867 total time=   0.0s\n",
      "[CV 5/5] END C=1, coef0=0.5, degree=3, kernel=poly;, score=0.802 total time=   0.0s\n",
      "[CV 1/5] END C=0.5, coef0=0.01, degree=1, kernel=sigmoid;, score=0.771 total time=   0.0s\n",
      "[CV 2/5] END C=0.5, coef0=0.01, degree=1, kernel=sigmoid;, score=0.756 total time=   0.0s\n",
      "[CV 3/5] END C=0.5, coef0=0.01, degree=1, kernel=sigmoid;, score=0.778 total time=   0.0s\n",
      "[CV 4/5] END C=0.5, coef0=0.01, degree=1, kernel=sigmoid;, score=0.822 total time=   0.0s\n",
      "[CV 5/5] END C=0.5, coef0=0.01, degree=1, kernel=sigmoid;, score=0.500 total time=   0.0s\n",
      "[CV 1/5] END C=0.01, coef0=0.5, degree=5, kernel=sigmoid;, score=0.757 total time=   0.0s\n",
      "[CV 2/5] END C=0.01, coef0=0.5, degree=5, kernel=sigmoid;, score=0.770 total time=   0.0s\n",
      "[CV 3/5] END C=0.01, coef0=0.5, degree=5, kernel=sigmoid;, score=0.763 total time=   0.0s\n",
      "[CV 4/5] END C=0.01, coef0=0.5, degree=5, kernel=sigmoid;, score=0.837 total time=   0.0s\n",
      "[CV 5/5] END C=0.01, coef0=0.5, degree=5, kernel=sigmoid;, score=0.571 total time=   0.0s\n",
      "[CV 1/5] END C=0.01, coef0=1, degree=3, kernel=sigmoid;, score=0.757 total time=   0.0s\n",
      "[CV 2/5] END C=0.01, coef0=1, degree=3, kernel=sigmoid;, score=0.763 total time=   0.0s\n",
      "[CV 3/5] END C=0.01, coef0=1, degree=3, kernel=sigmoid;, score=0.778 total time=   0.0s\n",
      "[CV 4/5] END C=0.01, coef0=1, degree=3, kernel=sigmoid;, score=0.830 total time=   0.0s\n",
      "[CV 5/5] END C=0.01, coef0=1, degree=3, kernel=sigmoid;, score=0.571 total time=   0.0s\n",
      "[CV 1/5] END C=0.01, coef0=1, degree=3, kernel=poly;, score=0.871 total time=   0.0s\n",
      "[CV 2/5] END C=0.01, coef0=1, degree=3, kernel=poly;, score=0.696 total time=   0.0s\n",
      "[CV 3/5] END C=0.01, coef0=1, degree=3, kernel=poly;, score=0.852 total time=   0.0s\n",
      "[CV 4/5] END C=0.01, coef0=1, degree=3, kernel=poly;, score=0.867 total time=   0.0s\n",
      "[CV 5/5] END C=0.01, coef0=1, degree=3, kernel=poly;, score=0.802 total time=   0.0s\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV 1/5] END C=0.01, coef0=0.01, degree=1, kernel=sigmoid;, score=0.707 total time=   0.0s\n",
      "[CV 2/5] END C=0.01, coef0=0.01, degree=1, kernel=sigmoid;, score=0.693 total time=   0.0s\n",
      "[CV 3/5] END C=0.01, coef0=0.01, degree=1, kernel=sigmoid;, score=0.719 total time=   0.0s\n",
      "[CV 4/5] END C=0.01, coef0=0.01, degree=1, kernel=sigmoid;, score=0.778 total time=   0.0s\n",
      "[CV 5/5] END C=0.01, coef0=0.01, degree=1, kernel=sigmoid;, score=0.516 total time=   0.0s\n",
      "[CV 1/5] END C=0.5, coef0=0.5, degree=3, kernel=poly;, score=0.786 total time=   0.0s\n",
      "[CV 2/5] END C=0.5, coef0=0.5, degree=3, kernel=poly;, score=0.686 total time=   0.0s\n",
      "[CV 3/5] END C=0.5, coef0=0.5, degree=3, kernel=poly;, score=0.630 total time=   0.0s\n",
      "[CV 4/5] END C=0.5, coef0=0.5, degree=3, kernel=poly;, score=0.941 total time=   0.0s\n",
      "[CV 5/5] END C=0.5, coef0=0.5, degree=3, kernel=poly;, score=0.881 total time=   0.0s\n",
      "[CV 1/5] END C=0.5, coef0=0.01, degree=1, kernel=poly;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END C=0.5, coef0=0.01, degree=1, kernel=poly;, score=0.893 total time=   0.0s\n",
      "[CV 3/5] END C=0.5, coef0=0.01, degree=1, kernel=poly;, score=0.696 total time=   0.0s\n",
      "[CV 4/5] END C=0.5, coef0=0.01, degree=1, kernel=poly;, score=0.993 total time=   0.0s\n",
      "[CV 5/5] END C=0.5, coef0=0.01, degree=1, kernel=poly;, score=0.960 total time=   0.0s\n",
      "[CV 1/5] END C=0.5, coef0=1, degree=3, kernel=poly;, score=0.793 total time=   0.0s\n",
      "[CV 2/5] END C=0.5, coef0=1, degree=3, kernel=poly;, score=0.743 total time=   0.0s\n",
      "[CV 3/5] END C=0.5, coef0=1, degree=3, kernel=poly;, score=0.644 total time=   0.0s\n",
      "[CV 4/5] END C=0.5, coef0=1, degree=3, kernel=poly;, score=0.970 total time=   0.0s\n",
      "[CV 5/5] END C=0.5, coef0=1, degree=3, kernel=poly;, score=0.960 total time=   0.0s\n",
      "[CV 1/5] END C=0.01, coef0=0.5, degree=1, kernel=rbf;, score=0.793 total time=   0.0s\n",
      "[CV 2/5] END C=0.01, coef0=0.5, degree=1, kernel=rbf;, score=0.793 total time=   0.0s\n",
      "[CV 3/5] END C=0.01, coef0=0.5, degree=1, kernel=rbf;, score=0.689 total time=   0.0s\n",
      "[CV 4/5] END C=0.01, coef0=0.5, degree=1, kernel=rbf;, score=0.978 total time=   0.0s\n",
      "[CV 5/5] END C=0.01, coef0=0.5, degree=1, kernel=rbf;, score=0.921 total time=   0.0s\n",
      "[CV 1/5] END C=0.01, coef0=1, degree=5, kernel=sigmoid;, score=0.714 total time=   0.0s\n",
      "[CV 2/5] END C=0.01, coef0=1, degree=5, kernel=sigmoid;, score=0.693 total time=   0.0s\n",
      "[CV 3/5] END C=0.01, coef0=1, degree=5, kernel=sigmoid;, score=0.726 total time=   0.0s\n",
      "[CV 4/5] END C=0.01, coef0=1, degree=5, kernel=sigmoid;, score=0.696 total time=   0.0s\n",
      "[CV 5/5] END C=0.01, coef0=1, degree=5, kernel=sigmoid;, score=0.548 total time=   0.0s\n",
      "[CV 1/5] END C=0.01, coef0=0.5, degree=1, kernel=sigmoid;, score=0.729 total time=   0.0s\n",
      "[CV 2/5] END C=0.01, coef0=0.5, degree=1, kernel=sigmoid;, score=0.693 total time=   0.0s\n",
      "[CV 3/5] END C=0.01, coef0=0.5, degree=1, kernel=sigmoid;, score=0.726 total time=   0.0s\n",
      "[CV 4/5] END C=0.01, coef0=0.5, degree=1, kernel=sigmoid;, score=0.711 total time=   0.0s\n",
      "[CV 5/5] END C=0.01, coef0=0.5, degree=1, kernel=sigmoid;, score=0.532 total time=   0.0s\n",
      "[CV 1/5] END C=1, coef0=1, degree=1, kernel=rbf;, score=0.793 total time=   0.0s\n",
      "[CV 2/5] END C=1, coef0=1, degree=1, kernel=rbf;, score=0.736 total time=   0.0s\n",
      "[CV 3/5] END C=1, coef0=1, degree=1, kernel=rbf;, score=0.681 total time=   0.0s\n",
      "[CV 4/5] END C=1, coef0=1, degree=1, kernel=rbf;, score=0.985 total time=   0.0s\n",
      "[CV 5/5] END C=1, coef0=1, degree=1, kernel=rbf;, score=0.937 total time=   0.0s\n",
      "[CV 1/5] END C=0.5, coef0=1, degree=1, kernel=sigmoid;, score=0.714 total time=   0.0s\n",
      "[CV 2/5] END C=0.5, coef0=1, degree=1, kernel=sigmoid;, score=0.693 total time=   0.0s\n",
      "[CV 3/5] END C=0.5, coef0=1, degree=1, kernel=sigmoid;, score=0.726 total time=   0.0s\n",
      "[CV 4/5] END C=0.5, coef0=1, degree=1, kernel=sigmoid;, score=0.763 total time=   0.0s\n",
      "[CV 5/5] END C=0.5, coef0=1, degree=1, kernel=sigmoid;, score=0.556 total time=   0.0s\n",
      "[CV 1/5] END C=0.01, coef0=1, degree=3, kernel=sigmoid;, score=0.714 total time=   0.0s\n",
      "[CV 2/5] END C=0.01, coef0=1, degree=3, kernel=sigmoid;, score=0.693 total time=   0.0s\n",
      "[CV 3/5] END C=0.01, coef0=1, degree=3, kernel=sigmoid;, score=0.726 total time=   0.0s\n",
      "[CV 4/5] END C=0.01, coef0=1, degree=3, kernel=sigmoid;, score=0.696 total time=   0.0s\n",
      "[CV 5/5] END C=0.01, coef0=1, degree=3, kernel=sigmoid;, score=0.548 total time=   0.0s\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV 1/5] END C=0.01, coef0=0.01, degree=5, kernel=poly;, score=0.586 total time=   0.0s\n",
      "[CV 2/5] END C=0.01, coef0=0.01, degree=5, kernel=poly;, score=0.364 total time=   0.0s\n",
      "[CV 3/5] END C=0.01, coef0=0.01, degree=5, kernel=poly;, score=0.526 total time=   0.0s\n",
      "[CV 4/5] END C=0.01, coef0=0.01, degree=5, kernel=poly;, score=0.793 total time=   0.0s\n",
      "[CV 5/5] END C=0.01, coef0=0.01, degree=5, kernel=poly;, score=0.785 total time=   0.0s\n",
      "[CV 1/5] END C=1, coef0=0.01, degree=5, kernel=sigmoid;, score=0.643 total time=   0.0s\n",
      "[CV 2/5] END C=1, coef0=0.01, degree=5, kernel=sigmoid;, score=0.614 total time=   0.0s\n",
      "[CV 3/5] END C=1, coef0=0.01, degree=5, kernel=sigmoid;, score=0.763 total time=   0.0s\n",
      "[CV 4/5] END C=1, coef0=0.01, degree=5, kernel=sigmoid;, score=0.652 total time=   0.0s\n",
      "[CV 5/5] END C=1, coef0=0.01, degree=5, kernel=sigmoid;, score=0.667 total time=   0.0s\n",
      "[CV 1/5] END C=0.5, coef0=0.01, degree=1, kernel=sigmoid;, score=0.643 total time=   0.0s\n",
      "[CV 2/5] END C=0.5, coef0=0.01, degree=1, kernel=sigmoid;, score=0.614 total time=   0.0s\n",
      "[CV 3/5] END C=0.5, coef0=0.01, degree=1, kernel=sigmoid;, score=0.763 total time=   0.0s\n",
      "[CV 4/5] END C=0.5, coef0=0.01, degree=1, kernel=sigmoid;, score=0.652 total time=   0.0s\n",
      "[CV 5/5] END C=0.5, coef0=0.01, degree=1, kernel=sigmoid;, score=0.667 total time=   0.0s\n",
      "[CV 1/5] END C=0.01, coef0=0.01, degree=5, kernel=sigmoid;, score=0.650 total time=   0.0s\n",
      "[CV 2/5] END C=0.01, coef0=0.01, degree=5, kernel=sigmoid;, score=0.629 total time=   0.0s\n",
      "[CV 3/5] END C=0.01, coef0=0.01, degree=5, kernel=sigmoid;, score=0.763 total time=   0.0s\n",
      "[CV 4/5] END C=0.01, coef0=0.01, degree=5, kernel=sigmoid;, score=0.578 total time=   0.0s\n",
      "[CV 5/5] END C=0.01, coef0=0.01, degree=5, kernel=sigmoid;, score=0.667 total time=   0.0s\n",
      "[CV 1/5] END C=0.01, coef0=0.5, degree=1, kernel=rbf;, score=0.621 total time=   0.0s\n",
      "[CV 2/5] END C=0.01, coef0=0.5, degree=1, kernel=rbf;, score=0.614 total time=   0.0s\n",
      "[CV 3/5] END C=0.01, coef0=0.5, degree=1, kernel=rbf;, score=0.770 total time=   0.0s\n",
      "[CV 4/5] END C=0.01, coef0=0.5, degree=1, kernel=rbf;, score=0.770 total time=   0.0s\n",
      "[CV 5/5] END C=0.01, coef0=0.5, degree=1, kernel=rbf;, score=1.000 total time=   0.0s\n",
      "[CV 1/5] END C=0.5, coef0=1, degree=1, kernel=rbf;, score=0.629 total time=   0.0s\n",
      "[CV 2/5] END C=0.5, coef0=1, degree=1, kernel=rbf;, score=0.593 total time=   0.0s\n",
      "[CV 3/5] END C=0.5, coef0=1, degree=1, kernel=rbf;, score=0.756 total time=   0.0s\n",
      "[CV 4/5] END C=0.5, coef0=1, degree=1, kernel=rbf;, score=0.785 total time=   0.0s\n",
      "[CV 5/5] END C=0.5, coef0=1, degree=1, kernel=rbf;, score=0.948 total time=   0.0s\n",
      "[CV 1/5] END C=0.01, coef0=0.5, degree=3, kernel=rbf;, score=0.621 total time=   0.0s\n",
      "[CV 2/5] END C=0.01, coef0=0.5, degree=3, kernel=rbf;, score=0.614 total time=   0.0s\n",
      "[CV 3/5] END C=0.01, coef0=0.5, degree=3, kernel=rbf;, score=0.770 total time=   0.0s\n",
      "[CV 4/5] END C=0.01, coef0=0.5, degree=3, kernel=rbf;, score=0.770 total time=   0.0s\n",
      "[CV 5/5] END C=0.01, coef0=0.5, degree=3, kernel=rbf;, score=1.000 total time=   0.0s\n",
      "[CV 1/5] END C=0.5, coef0=1, degree=3, kernel=poly;, score=0.679 total time=   0.0s\n",
      "[CV 2/5] END C=0.5, coef0=1, degree=3, kernel=poly;, score=0.500 total time=   0.0s\n",
      "[CV 3/5] END C=0.5, coef0=1, degree=3, kernel=poly;, score=0.763 total time=   0.0s\n",
      "[CV 4/5] END C=0.5, coef0=1, degree=3, kernel=poly;, score=0.770 total time=   0.0s\n",
      "[CV 5/5] END C=0.5, coef0=1, degree=3, kernel=poly;, score=0.911 total time=   0.0s\n",
      "[CV 1/5] END C=1, coef0=0.5, degree=3, kernel=sigmoid;, score=0.614 total time=   0.0s\n",
      "[CV 2/5] END C=1, coef0=0.5, degree=3, kernel=sigmoid;, score=0.614 total time=   0.0s\n",
      "[CV 3/5] END C=1, coef0=0.5, degree=3, kernel=sigmoid;, score=0.763 total time=   0.0s\n",
      "[CV 4/5] END C=1, coef0=0.5, degree=3, kernel=sigmoid;, score=0.652 total time=   0.0s\n",
      "[CV 5/5] END C=1, coef0=0.5, degree=3, kernel=sigmoid;, score=0.681 total time=   0.0s\n",
      "[CV 1/5] END C=0.5, coef0=0.5, degree=1, kernel=poly;, score=0.650 total time=   0.0s\n",
      "[CV 2/5] END C=0.5, coef0=0.5, degree=1, kernel=poly;, score=0.700 total time=   0.0s\n",
      "[CV 3/5] END C=0.5, coef0=0.5, degree=1, kernel=poly;, score=0.741 total time=   0.0s\n",
      "[CV 4/5] END C=0.5, coef0=0.5, degree=1, kernel=poly;, score=0.985 total time=   0.0s\n",
      "[CV 5/5] END C=0.5, coef0=0.5, degree=1, kernel=poly;, score=1.000 total time=   0.0s\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV 1/5] END C=1, coef0=0.01, degree=3, kernel=sigmoid;, score=0.786 total time=   0.0s\n",
      "[CV 2/5] END C=1, coef0=0.01, degree=3, kernel=sigmoid;, score=0.771 total time=   0.0s\n",
      "[CV 3/5] END C=1, coef0=0.01, degree=3, kernel=sigmoid;, score=0.526 total time=   0.0s\n",
      "[CV 4/5] END C=1, coef0=0.01, degree=3, kernel=sigmoid;, score=0.852 total time=   0.0s\n",
      "[CV 5/5] END C=1, coef0=0.01, degree=3, kernel=sigmoid;, score=0.807 total time=   0.0s\n",
      "[CV 1/5] END C=0.5, coef0=1, degree=3, kernel=sigmoid;, score=0.771 total time=   0.0s\n",
      "[CV 2/5] END C=0.5, coef0=1, degree=3, kernel=sigmoid;, score=0.714 total time=   0.0s\n",
      "[CV 3/5] END C=0.5, coef0=1, degree=3, kernel=sigmoid;, score=0.585 total time=   0.0s\n",
      "[CV 4/5] END C=0.5, coef0=1, degree=3, kernel=sigmoid;, score=0.837 total time=   0.0s\n",
      "[CV 5/5] END C=0.5, coef0=1, degree=3, kernel=sigmoid;, score=0.763 total time=   0.0s\n",
      "[CV 1/5] END C=0.01, coef0=0.5, degree=3, kernel=rbf;, score=0.814 total time=   0.0s\n",
      "[CV 2/5] END C=0.01, coef0=0.5, degree=3, kernel=rbf;, score=0.721 total time=   0.0s\n",
      "[CV 3/5] END C=0.01, coef0=0.5, degree=3, kernel=rbf;, score=0.615 total time=   0.0s\n",
      "[CV 4/5] END C=0.01, coef0=0.5, degree=3, kernel=rbf;, score=0.748 total time=   0.0s\n",
      "[CV 5/5] END C=0.01, coef0=0.5, degree=3, kernel=rbf;, score=0.904 total time=   0.0s\n",
      "[CV 1/5] END C=0.01, coef0=0.01, degree=1, kernel=rbf;, score=0.814 total time=   0.0s\n",
      "[CV 2/5] END C=0.01, coef0=0.01, degree=1, kernel=rbf;, score=0.721 total time=   0.0s\n",
      "[CV 3/5] END C=0.01, coef0=0.01, degree=1, kernel=rbf;, score=0.615 total time=   0.0s\n",
      "[CV 4/5] END C=0.01, coef0=0.01, degree=1, kernel=rbf;, score=0.748 total time=   0.0s\n",
      "[CV 5/5] END C=0.01, coef0=0.01, degree=1, kernel=rbf;, score=0.904 total time=   0.0s\n",
      "[CV 1/5] END C=1, coef0=0.01, degree=3, kernel=poly;, score=0.500 total time=   0.0s\n",
      "[CV 2/5] END C=1, coef0=0.01, degree=3, kernel=poly;, score=0.700 total time=   0.0s\n",
      "[CV 3/5] END C=1, coef0=0.01, degree=3, kernel=poly;, score=0.526 total time=   0.0s\n",
      "[CV 4/5] END C=1, coef0=0.01, degree=3, kernel=poly;, score=0.741 total time=   0.0s\n",
      "[CV 5/5] END C=1, coef0=0.01, degree=3, kernel=poly;, score=0.889 total time=   0.0s\n",
      "[CV 1/5] END C=0.5, coef0=1, degree=5, kernel=rbf;, score=0.793 total time=   0.0s\n",
      "[CV 2/5] END C=0.5, coef0=1, degree=5, kernel=rbf;, score=0.743 total time=   0.0s\n",
      "[CV 3/5] END C=0.5, coef0=1, degree=5, kernel=rbf;, score=0.637 total time=   0.0s\n",
      "[CV 4/5] END C=0.5, coef0=1, degree=5, kernel=rbf;, score=0.756 total time=   0.0s\n",
      "[CV 5/5] END C=0.5, coef0=1, degree=5, kernel=rbf;, score=0.919 total time=   0.0s\n",
      "[CV 1/5] END C=0.5, coef0=1, degree=5, kernel=poly;, score=0.864 total time=   0.0s\n",
      "[CV 2/5] END C=0.5, coef0=1, degree=5, kernel=poly;, score=0.764 total time=   0.0s\n",
      "[CV 3/5] END C=0.5, coef0=1, degree=5, kernel=poly;, score=0.607 total time=   0.0s\n",
      "[CV 4/5] END C=0.5, coef0=1, degree=5, kernel=poly;, score=0.578 total time=   0.0s\n",
      "[CV 5/5] END C=0.5, coef0=1, degree=5, kernel=poly;, score=0.911 total time=   0.0s\n",
      "[CV 1/5] END C=1, coef0=0.5, degree=5, kernel=rbf;, score=0.793 total time=   0.0s\n",
      "[CV 2/5] END C=1, coef0=0.5, degree=5, kernel=rbf;, score=0.743 total time=   0.0s\n",
      "[CV 3/5] END C=1, coef0=0.5, degree=5, kernel=rbf;, score=0.637 total time=   0.0s\n",
      "[CV 4/5] END C=1, coef0=0.5, degree=5, kernel=rbf;, score=0.756 total time=   0.0s\n",
      "[CV 5/5] END C=1, coef0=0.5, degree=5, kernel=rbf;, score=0.919 total time=   0.0s\n",
      "[CV 1/5] END C=0.5, coef0=0.01, degree=5, kernel=sigmoid;, score=0.786 total time=   0.0s\n",
      "[CV 2/5] END C=0.5, coef0=0.01, degree=5, kernel=sigmoid;, score=0.771 total time=   0.0s\n",
      "[CV 3/5] END C=0.5, coef0=0.01, degree=5, kernel=sigmoid;, score=0.548 total time=   0.0s\n",
      "[CV 4/5] END C=0.5, coef0=0.01, degree=5, kernel=sigmoid;, score=0.852 total time=   0.0s\n",
      "[CV 5/5] END C=0.5, coef0=0.01, degree=5, kernel=sigmoid;, score=0.807 total time=   0.0s\n",
      "[CV 1/5] END C=0.01, coef0=1, degree=5, kernel=rbf;, score=0.814 total time=   0.0s\n",
      "[CV 2/5] END C=0.01, coef0=1, degree=5, kernel=rbf;, score=0.721 total time=   0.0s\n",
      "[CV 3/5] END C=0.01, coef0=1, degree=5, kernel=rbf;, score=0.615 total time=   0.0s\n",
      "[CV 4/5] END C=0.01, coef0=1, degree=5, kernel=rbf;, score=0.748 total time=   0.0s\n",
      "[CV 5/5] END C=0.01, coef0=1, degree=5, kernel=rbf;, score=0.904 total time=   0.0s\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV 1/5] END C=0.01, coef0=0.5, degree=3, kernel=rbf;, score=0.785 total time=   0.0s\n",
      "[CV 2/5] END C=0.01, coef0=0.5, degree=3, kernel=rbf;, score=0.822 total time=   0.0s\n",
      "[CV 3/5] END C=0.01, coef0=0.5, degree=3, kernel=rbf;, score=0.778 total time=   0.0s\n",
      "[CV 4/5] END C=0.01, coef0=0.5, degree=3, kernel=rbf;, score=0.821 total time=   0.0s\n",
      "[CV 5/5] END C=0.01, coef0=0.5, degree=3, kernel=rbf;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END C=0.5, coef0=0.01, degree=3, kernel=sigmoid;, score=0.793 total time=   0.0s\n",
      "[CV 2/5] END C=0.5, coef0=0.01, degree=3, kernel=sigmoid;, score=0.607 total time=   0.0s\n",
      "[CV 3/5] END C=0.5, coef0=0.01, degree=3, kernel=sigmoid;, score=0.778 total time=   0.0s\n",
      "[CV 4/5] END C=0.5, coef0=0.01, degree=3, kernel=sigmoid;, score=0.736 total time=   0.0s\n",
      "[CV 5/5] END C=0.5, coef0=0.01, degree=3, kernel=sigmoid;, score=0.607 total time=   0.0s\n",
      "[CV 1/5] END C=0.01, coef0=0.5, degree=1, kernel=rbf;, score=0.785 total time=   0.0s\n",
      "[CV 2/5] END C=0.01, coef0=0.5, degree=1, kernel=rbf;, score=0.822 total time=   0.0s\n",
      "[CV 3/5] END C=0.01, coef0=0.5, degree=1, kernel=rbf;, score=0.778 total time=   0.0s\n",
      "[CV 4/5] END C=0.01, coef0=0.5, degree=1, kernel=rbf;, score=0.821 total time=   0.0s\n",
      "[CV 5/5] END C=0.01, coef0=0.5, degree=1, kernel=rbf;, score=0.814 total time=   0.0s\n",
      "[CV 1/5] END C=0.5, coef0=0.01, degree=3, kernel=poly;, score=0.674 total time=   0.0s\n",
      "[CV 2/5] END C=0.5, coef0=0.01, degree=3, kernel=poly;, score=0.711 total time=   0.0s\n",
      "[CV 3/5] END C=0.5, coef0=0.01, degree=3, kernel=poly;, score=0.600 total time=   0.0s\n",
      "[CV 4/5] END C=0.5, coef0=0.01, degree=3, kernel=poly;, score=0.564 total time=   0.0s\n",
      "[CV 5/5] END C=0.5, coef0=0.01, degree=3, kernel=poly;, score=0.743 total time=   0.0s\n",
      "[CV 1/5] END C=0.5, coef0=0.5, degree=1, kernel=sigmoid;, score=0.793 total time=   0.0s\n",
      "[CV 2/5] END C=0.5, coef0=0.5, degree=1, kernel=sigmoid;, score=0.607 total time=   0.0s\n",
      "[CV 3/5] END C=0.5, coef0=0.5, degree=1, kernel=sigmoid;, score=0.778 total time=   0.0s\n",
      "[CV 4/5] END C=0.5, coef0=0.5, degree=1, kernel=sigmoid;, score=0.779 total time=   0.0s\n",
      "[CV 5/5] END C=0.5, coef0=0.5, degree=1, kernel=sigmoid;, score=0.907 total time=   0.0s\n",
      "[CV 1/5] END C=0.01, coef0=1, degree=3, kernel=poly;, score=0.852 total time=   0.0s\n",
      "[CV 2/5] END C=0.01, coef0=1, degree=3, kernel=poly;, score=0.748 total time=   0.0s\n",
      "[CV 3/5] END C=0.01, coef0=1, degree=3, kernel=poly;, score=0.704 total time=   0.0s\n",
      "[CV 4/5] END C=0.01, coef0=1, degree=3, kernel=poly;, score=0.771 total time=   0.0s\n",
      "[CV 5/5] END C=0.01, coef0=1, degree=3, kernel=poly;, score=0.886 total time=   0.0s\n",
      "[CV 1/5] END C=0.5, coef0=0.5, degree=3, kernel=sigmoid;, score=0.793 total time=   0.0s\n",
      "[CV 2/5] END C=0.5, coef0=0.5, degree=3, kernel=sigmoid;, score=0.607 total time=   0.0s\n",
      "[CV 3/5] END C=0.5, coef0=0.5, degree=3, kernel=sigmoid;, score=0.778 total time=   0.0s\n",
      "[CV 4/5] END C=0.5, coef0=0.5, degree=3, kernel=sigmoid;, score=0.779 total time=   0.0s\n",
      "[CV 5/5] END C=0.5, coef0=0.5, degree=3, kernel=sigmoid;, score=0.907 total time=   0.0s\n",
      "[CV 1/5] END C=0.01, coef0=0.01, degree=1, kernel=sigmoid;, score=0.741 total time=   0.0s\n",
      "[CV 2/5] END C=0.01, coef0=0.01, degree=1, kernel=sigmoid;, score=0.593 total time=   0.0s\n",
      "[CV 3/5] END C=0.01, coef0=0.01, degree=1, kernel=sigmoid;, score=0.778 total time=   0.0s\n",
      "[CV 4/5] END C=0.01, coef0=0.01, degree=1, kernel=sigmoid;, score=0.736 total time=   0.0s\n",
      "[CV 5/5] END C=0.01, coef0=0.01, degree=1, kernel=sigmoid;, score=0.571 total time=   0.0s\n",
      "[CV 1/5] END C=1, coef0=1, degree=1, kernel=poly;, score=0.852 total time=   0.0s\n",
      "[CV 2/5] END C=1, coef0=1, degree=1, kernel=poly;, score=0.941 total time=   0.0s\n",
      "[CV 3/5] END C=1, coef0=1, degree=1, kernel=poly;, score=0.867 total time=   0.0s\n",
      "[CV 4/5] END C=1, coef0=1, degree=1, kernel=poly;, score=0.829 total time=   0.0s\n",
      "[CV 5/5] END C=1, coef0=1, degree=1, kernel=poly;, score=0.979 total time=   0.0s\n",
      "[CV 1/5] END C=0.01, coef0=1, degree=5, kernel=rbf;, score=0.785 total time=   0.0s\n",
      "[CV 2/5] END C=0.01, coef0=1, degree=5, kernel=rbf;, score=0.822 total time=   0.0s\n",
      "[CV 3/5] END C=0.01, coef0=1, degree=5, kernel=rbf;, score=0.778 total time=   0.0s\n",
      "[CV 4/5] END C=0.01, coef0=1, degree=5, kernel=rbf;, score=0.821 total time=   0.0s\n",
      "[CV 5/5] END C=0.01, coef0=1, degree=5, kernel=rbf;, score=0.814 total time=   0.0s\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV 1/5] END C=0.01, coef0=1, degree=1, kernel=rbf;, score=0.879 total time=   0.0s\n",
      "[CV 2/5] END C=0.01, coef0=1, degree=1, kernel=rbf;, score=0.821 total time=   0.0s\n",
      "[CV 3/5] END C=0.01, coef0=1, degree=1, kernel=rbf;, score=0.511 total time=   0.0s\n",
      "[CV 4/5] END C=0.01, coef0=1, degree=1, kernel=rbf;, score=0.807 total time=   0.0s\n",
      "[CV 5/5] END C=0.01, coef0=1, degree=1, kernel=rbf;, score=0.956 total time=   0.0s\n",
      "[CV 1/5] END C=0.5, coef0=0.01, degree=3, kernel=poly;, score=0.650 total time=   0.0s\n",
      "[CV 2/5] END C=0.5, coef0=0.01, degree=3, kernel=poly;, score=0.657 total time=   0.0s\n",
      "[CV 3/5] END C=0.5, coef0=0.01, degree=3, kernel=poly;, score=0.467 total time=   0.0s\n",
      "[CV 4/5] END C=0.5, coef0=0.01, degree=3, kernel=poly;, score=0.867 total time=   0.0s\n",
      "[CV 5/5] END C=0.5, coef0=0.01, degree=3, kernel=poly;, score=0.689 total time=   0.0s\n",
      "[CV 1/5] END C=0.01, coef0=1, degree=5, kernel=sigmoid;, score=0.714 total time=   0.0s\n",
      "[CV 2/5] END C=0.01, coef0=1, degree=5, kernel=sigmoid;, score=0.707 total time=   0.0s\n",
      "[CV 3/5] END C=0.01, coef0=1, degree=5, kernel=sigmoid;, score=0.667 total time=   0.0s\n",
      "[CV 4/5] END C=0.01, coef0=1, degree=5, kernel=sigmoid;, score=0.867 total time=   0.0s\n",
      "[CV 5/5] END C=0.01, coef0=1, degree=5, kernel=sigmoid;, score=0.867 total time=   0.0s\n",
      "[CV 1/5] END C=0.01, coef0=0.5, degree=3, kernel=poly;, score=0.743 total time=   0.0s\n",
      "[CV 2/5] END C=0.01, coef0=0.5, degree=3, kernel=poly;, score=0.793 total time=   0.0s\n",
      "[CV 3/5] END C=0.01, coef0=0.5, degree=3, kernel=poly;, score=0.519 total time=   0.0s\n",
      "[CV 4/5] END C=0.01, coef0=0.5, degree=3, kernel=poly;, score=0.881 total time=   0.0s\n",
      "[CV 5/5] END C=0.01, coef0=0.5, degree=3, kernel=poly;, score=0.881 total time=   0.0s\n",
      "[CV 1/5] END C=0.01, coef0=0.5, degree=1, kernel=rbf;, score=0.879 total time=   0.0s\n",
      "[CV 2/5] END C=0.01, coef0=0.5, degree=1, kernel=rbf;, score=0.821 total time=   0.0s\n",
      "[CV 3/5] END C=0.01, coef0=0.5, degree=1, kernel=rbf;, score=0.511 total time=   0.0s\n",
      "[CV 4/5] END C=0.01, coef0=0.5, degree=1, kernel=rbf;, score=0.807 total time=   0.0s\n",
      "[CV 5/5] END C=0.01, coef0=0.5, degree=1, kernel=rbf;, score=0.956 total time=   0.0s\n",
      "[CV 1/5] END C=0.5, coef0=1, degree=1, kernel=sigmoid;, score=0.729 total time=   0.0s\n",
      "[CV 2/5] END C=0.5, coef0=1, degree=1, kernel=sigmoid;, score=0.700 total time=   0.0s\n",
      "[CV 3/5] END C=0.5, coef0=1, degree=1, kernel=sigmoid;, score=0.667 total time=   0.0s\n",
      "[CV 4/5] END C=0.5, coef0=1, degree=1, kernel=sigmoid;, score=0.874 total time=   0.0s\n",
      "[CV 5/5] END C=0.5, coef0=1, degree=1, kernel=sigmoid;, score=0.711 total time=   0.0s\n",
      "[CV 1/5] END C=0.01, coef0=0.01, degree=1, kernel=rbf;, score=0.879 total time=   0.0s\n",
      "[CV 2/5] END C=0.01, coef0=0.01, degree=1, kernel=rbf;, score=0.821 total time=   0.0s\n",
      "[CV 3/5] END C=0.01, coef0=0.01, degree=1, kernel=rbf;, score=0.511 total time=   0.0s\n",
      "[CV 4/5] END C=0.01, coef0=0.01, degree=1, kernel=rbf;, score=0.807 total time=   0.0s\n",
      "[CV 5/5] END C=0.01, coef0=0.01, degree=1, kernel=rbf;, score=0.956 total time=   0.0s\n",
      "[CV 1/5] END C=0.01, coef0=0.5, degree=5, kernel=rbf;, score=0.879 total time=   0.0s\n",
      "[CV 2/5] END C=0.01, coef0=0.5, degree=5, kernel=rbf;, score=0.821 total time=   0.0s\n",
      "[CV 3/5] END C=0.01, coef0=0.5, degree=5, kernel=rbf;, score=0.511 total time=   0.0s\n",
      "[CV 4/5] END C=0.01, coef0=0.5, degree=5, kernel=rbf;, score=0.807 total time=   0.0s\n",
      "[CV 5/5] END C=0.01, coef0=0.5, degree=5, kernel=rbf;, score=0.956 total time=   0.0s\n",
      "[CV 1/5] END C=1, coef0=1, degree=5, kernel=rbf;, score=0.886 total time=   0.0s\n",
      "[CV 2/5] END C=1, coef0=1, degree=5, kernel=rbf;, score=0.814 total time=   0.0s\n",
      "[CV 3/5] END C=1, coef0=1, degree=5, kernel=rbf;, score=0.504 total time=   0.0s\n",
      "[CV 4/5] END C=1, coef0=1, degree=5, kernel=rbf;, score=0.807 total time=   0.0s\n",
      "[CV 5/5] END C=1, coef0=1, degree=5, kernel=rbf;, score=0.941 total time=   0.0s\n",
      "[CV 1/5] END C=0.01, coef0=0.01, degree=3, kernel=rbf;, score=0.879 total time=   0.0s\n",
      "[CV 2/5] END C=0.01, coef0=0.01, degree=3, kernel=rbf;, score=0.821 total time=   0.0s\n",
      "[CV 3/5] END C=0.01, coef0=0.01, degree=3, kernel=rbf;, score=0.511 total time=   0.0s\n",
      "[CV 4/5] END C=0.01, coef0=0.01, degree=3, kernel=rbf;, score=0.807 total time=   0.0s\n",
      "[CV 5/5] END C=0.01, coef0=0.01, degree=3, kernel=rbf;, score=0.956 total time=   0.0s\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV 1/5] END C=0.01, coef0=0.5, degree=5, kernel=sigmoid;, score=0.779 total time=   0.0s\n",
      "[CV 2/5] END C=0.01, coef0=0.5, degree=5, kernel=sigmoid;, score=0.543 total time=   0.0s\n",
      "[CV 3/5] END C=0.01, coef0=0.5, degree=5, kernel=sigmoid;, score=0.341 total time=   0.0s\n",
      "[CV 4/5] END C=0.01, coef0=0.5, degree=5, kernel=sigmoid;, score=0.904 total time=   0.0s\n",
      "[CV 5/5] END C=0.01, coef0=0.5, degree=5, kernel=sigmoid;, score=0.889 total time=   0.0s\n",
      "[CV 1/5] END C=0.01, coef0=0.01, degree=5, kernel=sigmoid;, score=0.764 total time=   0.0s\n",
      "[CV 2/5] END C=0.01, coef0=0.01, degree=5, kernel=sigmoid;, score=0.536 total time=   0.0s\n",
      "[CV 3/5] END C=0.01, coef0=0.01, degree=5, kernel=sigmoid;, score=0.341 total time=   0.0s\n",
      "[CV 4/5] END C=0.01, coef0=0.01, degree=5, kernel=sigmoid;, score=0.852 total time=   0.0s\n",
      "[CV 5/5] END C=0.01, coef0=0.01, degree=5, kernel=sigmoid;, score=0.822 total time=   0.0s\n",
      "[CV 1/5] END C=1, coef0=0.01, degree=3, kernel=sigmoid;, score=0.771 total time=   0.0s\n",
      "[CV 2/5] END C=1, coef0=0.01, degree=3, kernel=sigmoid;, score=0.536 total time=   0.0s\n",
      "[CV 3/5] END C=1, coef0=0.01, degree=3, kernel=sigmoid;, score=0.333 total time=   0.0s\n",
      "[CV 4/5] END C=1, coef0=0.01, degree=3, kernel=sigmoid;, score=0.896 total time=   0.0s\n",
      "[CV 5/5] END C=1, coef0=0.01, degree=3, kernel=sigmoid;, score=0.822 total time=   0.0s\n",
      "[CV 1/5] END C=0.5, coef0=1, degree=5, kernel=rbf;, score=0.857 total time=   0.0s\n",
      "[CV 2/5] END C=0.5, coef0=1, degree=5, kernel=rbf;, score=0.686 total time=   0.0s\n",
      "[CV 3/5] END C=0.5, coef0=1, degree=5, kernel=rbf;, score=0.556 total time=   0.0s\n",
      "[CV 4/5] END C=0.5, coef0=1, degree=5, kernel=rbf;, score=0.896 total time=   0.0s\n",
      "[CV 5/5] END C=0.5, coef0=1, degree=5, kernel=rbf;, score=0.926 total time=   0.0s\n",
      "[CV 1/5] END C=1, coef0=0.01, degree=3, kernel=rbf;, score=0.857 total time=   0.0s\n",
      "[CV 2/5] END C=1, coef0=0.01, degree=3, kernel=rbf;, score=0.686 total time=   0.0s\n",
      "[CV 3/5] END C=1, coef0=0.01, degree=3, kernel=rbf;, score=0.556 total time=   0.0s\n",
      "[CV 4/5] END C=1, coef0=0.01, degree=3, kernel=rbf;, score=0.896 total time=   0.0s\n",
      "[CV 5/5] END C=1, coef0=0.01, degree=3, kernel=rbf;, score=0.926 total time=   0.0s\n",
      "[CV 1/5] END C=0.01, coef0=1, degree=3, kernel=sigmoid;, score=0.786 total time=   0.0s\n",
      "[CV 2/5] END C=0.01, coef0=1, degree=3, kernel=sigmoid;, score=0.557 total time=   0.0s\n",
      "[CV 3/5] END C=0.01, coef0=1, degree=3, kernel=sigmoid;, score=0.341 total time=   0.0s\n",
      "[CV 4/5] END C=0.01, coef0=1, degree=3, kernel=sigmoid;, score=0.874 total time=   0.0s\n",
      "[CV 5/5] END C=0.01, coef0=1, degree=3, kernel=sigmoid;, score=0.793 total time=   0.0s\n",
      "[CV 1/5] END C=0.01, coef0=0.01, degree=3, kernel=rbf;, score=0.814 total time=   0.0s\n",
      "[CV 2/5] END C=0.01, coef0=0.01, degree=3, kernel=rbf;, score=0.707 total time=   0.0s\n",
      "[CV 3/5] END C=0.01, coef0=0.01, degree=3, kernel=rbf;, score=0.563 total time=   0.0s\n",
      "[CV 4/5] END C=0.01, coef0=0.01, degree=3, kernel=rbf;, score=0.889 total time=   0.0s\n",
      "[CV 5/5] END C=0.01, coef0=0.01, degree=3, kernel=rbf;, score=0.970 total time=   0.0s\n",
      "[CV 1/5] END C=0.5, coef0=0.5, degree=5, kernel=sigmoid;, score=0.771 total time=   0.0s\n",
      "[CV 2/5] END C=0.5, coef0=0.5, degree=5, kernel=sigmoid;, score=0.543 total time=   0.0s\n",
      "[CV 3/5] END C=0.5, coef0=0.5, degree=5, kernel=sigmoid;, score=0.341 total time=   0.0s\n",
      "[CV 4/5] END C=0.5, coef0=0.5, degree=5, kernel=sigmoid;, score=0.889 total time=   0.0s\n",
      "[CV 5/5] END C=0.5, coef0=0.5, degree=5, kernel=sigmoid;, score=0.881 total time=   0.0s\n",
      "[CV 1/5] END C=0.5, coef0=0.01, degree=1, kernel=rbf;, score=0.857 total time=   0.0s\n",
      "[CV 2/5] END C=0.5, coef0=0.01, degree=1, kernel=rbf;, score=0.686 total time=   0.0s\n",
      "[CV 3/5] END C=0.5, coef0=0.01, degree=1, kernel=rbf;, score=0.556 total time=   0.0s\n",
      "[CV 4/5] END C=0.5, coef0=0.01, degree=1, kernel=rbf;, score=0.896 total time=   0.0s\n",
      "[CV 5/5] END C=0.5, coef0=0.01, degree=1, kernel=rbf;, score=0.926 total time=   0.0s\n",
      "[CV 1/5] END C=1, coef0=0.01, degree=5, kernel=poly;, score=0.464 total time=   0.0s\n",
      "[CV 2/5] END C=1, coef0=0.01, degree=5, kernel=poly;, score=0.564 total time=   0.0s\n",
      "[CV 3/5] END C=1, coef0=0.01, degree=5, kernel=poly;, score=0.570 total time=   0.0s\n",
      "[CV 4/5] END C=1, coef0=0.01, degree=5, kernel=poly;, score=0.815 total time=   0.0s\n",
      "[CV 5/5] END C=1, coef0=0.01, degree=5, kernel=poly;, score=0.681 total time=   0.0s\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV 1/5] END C=1, coef0=0.01, degree=1, kernel=rbf;, score=0.871 total time=   0.0s\n",
      "[CV 2/5] END C=1, coef0=0.01, degree=1, kernel=rbf;, score=0.629 total time=   0.0s\n",
      "[CV 3/5] END C=1, coef0=0.01, degree=1, kernel=rbf;, score=0.644 total time=   0.0s\n",
      "[CV 4/5] END C=1, coef0=0.01, degree=1, kernel=rbf;, score=1.000 total time=   0.0s\n",
      "[CV 5/5] END C=1, coef0=0.01, degree=1, kernel=rbf;, score=0.852 total time=   0.0s\n",
      "[CV 1/5] END C=0.5, coef0=1, degree=1, kernel=sigmoid;, score=0.757 total time=   0.0s\n",
      "[CV 2/5] END C=0.5, coef0=1, degree=1, kernel=sigmoid;, score=0.550 total time=   0.0s\n",
      "[CV 3/5] END C=0.5, coef0=1, degree=1, kernel=sigmoid;, score=0.548 total time=   0.0s\n",
      "[CV 4/5] END C=0.5, coef0=1, degree=1, kernel=sigmoid;, score=1.000 total time=   0.0s\n",
      "[CV 5/5] END C=0.5, coef0=1, degree=1, kernel=sigmoid;, score=0.933 total time=   0.0s\n",
      "[CV 1/5] END C=0.01, coef0=0.01, degree=3, kernel=sigmoid;, score=0.843 total time=   0.0s\n",
      "[CV 2/5] END C=0.01, coef0=0.01, degree=3, kernel=sigmoid;, score=0.543 total time=   0.0s\n",
      "[CV 3/5] END C=0.01, coef0=0.01, degree=3, kernel=sigmoid;, score=0.548 total time=   0.0s\n",
      "[CV 4/5] END C=0.01, coef0=0.01, degree=3, kernel=sigmoid;, score=0.993 total time=   0.0s\n",
      "[CV 5/5] END C=0.01, coef0=0.01, degree=3, kernel=sigmoid;, score=0.793 total time=   0.0s\n",
      "[CV 1/5] END C=0.01, coef0=1, degree=5, kernel=sigmoid;, score=0.893 total time=   0.0s\n",
      "[CV 2/5] END C=0.01, coef0=1, degree=5, kernel=sigmoid;, score=0.543 total time=   0.0s\n",
      "[CV 3/5] END C=0.01, coef0=1, degree=5, kernel=sigmoid;, score=0.541 total time=   0.0s\n",
      "[CV 4/5] END C=0.01, coef0=1, degree=5, kernel=sigmoid;, score=1.000 total time=   0.0s\n",
      "[CV 5/5] END C=0.01, coef0=1, degree=5, kernel=sigmoid;, score=0.793 total time=   0.0s\n",
      "[CV 1/5] END C=1, coef0=1, degree=5, kernel=rbf;, score=0.871 total time=   0.0s\n",
      "[CV 2/5] END C=1, coef0=1, degree=5, kernel=rbf;, score=0.629 total time=   0.0s\n",
      "[CV 3/5] END C=1, coef0=1, degree=5, kernel=rbf;, score=0.644 total time=   0.0s\n",
      "[CV 4/5] END C=1, coef0=1, degree=5, kernel=rbf;, score=1.000 total time=   0.0s\n",
      "[CV 5/5] END C=1, coef0=1, degree=5, kernel=rbf;, score=0.852 total time=   0.0s\n",
      "[CV 1/5] END C=1, coef0=0.5, degree=1, kernel=rbf;, score=0.871 total time=   0.0s\n",
      "[CV 2/5] END C=1, coef0=0.5, degree=1, kernel=rbf;, score=0.629 total time=   0.0s\n",
      "[CV 3/5] END C=1, coef0=0.5, degree=1, kernel=rbf;, score=0.644 total time=   0.0s\n",
      "[CV 4/5] END C=1, coef0=0.5, degree=1, kernel=rbf;, score=1.000 total time=   0.0s\n",
      "[CV 5/5] END C=1, coef0=0.5, degree=1, kernel=rbf;, score=0.852 total time=   0.0s\n",
      "[CV 1/5] END C=1, coef0=0.5, degree=3, kernel=poly;, score=0.807 total time=   0.0s\n",
      "[CV 2/5] END C=1, coef0=0.5, degree=3, kernel=poly;, score=0.514 total time=   0.0s\n",
      "[CV 3/5] END C=1, coef0=0.5, degree=3, kernel=poly;, score=0.622 total time=   0.0s\n",
      "[CV 4/5] END C=1, coef0=0.5, degree=3, kernel=poly;, score=0.941 total time=   0.0s\n",
      "[CV 5/5] END C=1, coef0=0.5, degree=3, kernel=poly;, score=0.800 total time=   0.0s\n",
      "[CV 1/5] END C=0.01, coef0=1, degree=3, kernel=rbf;, score=0.871 total time=   0.0s\n",
      "[CV 2/5] END C=0.01, coef0=1, degree=3, kernel=rbf;, score=0.629 total time=   0.0s\n",
      "[CV 3/5] END C=0.01, coef0=1, degree=3, kernel=rbf;, score=0.644 total time=   0.0s\n",
      "[CV 4/5] END C=0.01, coef0=1, degree=3, kernel=rbf;, score=1.000 total time=   0.0s\n",
      "[CV 5/5] END C=0.01, coef0=1, degree=3, kernel=rbf;, score=0.785 total time=   0.0s\n",
      "[CV 1/5] END C=0.5, coef0=0.01, degree=1, kernel=poly;, score=0.886 total time=   0.0s\n",
      "[CV 2/5] END C=0.5, coef0=0.01, degree=1, kernel=poly;, score=0.757 total time=   0.0s\n",
      "[CV 3/5] END C=0.5, coef0=0.01, degree=1, kernel=poly;, score=0.630 total time=   0.0s\n",
      "[CV 4/5] END C=0.5, coef0=0.01, degree=1, kernel=poly;, score=1.000 total time=   0.0s\n",
      "[CV 5/5] END C=0.5, coef0=0.01, degree=1, kernel=poly;, score=0.956 total time=   0.0s\n",
      "[CV 1/5] END C=0.01, coef0=1, degree=5, kernel=poly;, score=0.800 total time=   0.0s\n",
      "[CV 2/5] END C=0.01, coef0=1, degree=5, kernel=poly;, score=0.707 total time=   0.0s\n",
      "[CV 3/5] END C=0.01, coef0=1, degree=5, kernel=poly;, score=0.630 total time=   0.0s\n",
      "[CV 4/5] END C=0.01, coef0=1, degree=5, kernel=poly;, score=0.941 total time=   0.0s\n",
      "[CV 5/5] END C=0.01, coef0=1, degree=5, kernel=poly;, score=0.704 total time=   0.0s\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV 1/5] END C=0.01, coef0=0.01, degree=5, kernel=poly;, score=0.350 total time=   0.0s\n",
      "[CV 2/5] END C=0.01, coef0=0.01, degree=5, kernel=poly;, score=0.536 total time=   0.0s\n",
      "[CV 3/5] END C=0.01, coef0=0.01, degree=5, kernel=poly;, score=0.519 total time=   0.0s\n",
      "[CV 4/5] END C=0.01, coef0=0.01, degree=5, kernel=poly;, score=0.763 total time=   0.0s\n",
      "[CV 5/5] END C=0.01, coef0=0.01, degree=5, kernel=poly;, score=0.748 total time=   0.0s\n",
      "[CV 1/5] END C=1, coef0=0.5, degree=3, kernel=sigmoid;, score=0.557 total time=   0.0s\n",
      "[CV 2/5] END C=1, coef0=0.5, degree=3, kernel=sigmoid;, score=0.664 total time=   0.0s\n",
      "[CV 3/5] END C=1, coef0=0.5, degree=3, kernel=sigmoid;, score=0.644 total time=   0.0s\n",
      "[CV 4/5] END C=1, coef0=0.5, degree=3, kernel=sigmoid;, score=0.956 total time=   0.0s\n",
      "[CV 5/5] END C=1, coef0=0.5, degree=3, kernel=sigmoid;, score=0.815 total time=   0.0s\n",
      "[CV 1/5] END C=1, coef0=0.5, degree=5, kernel=sigmoid;, score=0.557 total time=   0.0s\n",
      "[CV 2/5] END C=1, coef0=0.5, degree=5, kernel=sigmoid;, score=0.664 total time=   0.0s\n",
      "[CV 3/5] END C=1, coef0=0.5, degree=5, kernel=sigmoid;, score=0.644 total time=   0.0s\n",
      "[CV 4/5] END C=1, coef0=0.5, degree=5, kernel=sigmoid;, score=0.956 total time=   0.0s\n",
      "[CV 5/5] END C=1, coef0=0.5, degree=5, kernel=sigmoid;, score=0.815 total time=   0.0s\n",
      "[CV 1/5] END C=0.5, coef0=0.01, degree=3, kernel=poly;, score=0.421 total time=   0.0s\n",
      "[CV 2/5] END C=0.5, coef0=0.01, degree=3, kernel=poly;, score=0.593 total time=   0.0s\n",
      "[CV 3/5] END C=0.5, coef0=0.01, degree=3, kernel=poly;, score=0.504 total time=   0.0s\n",
      "[CV 4/5] END C=0.5, coef0=0.01, degree=3, kernel=poly;, score=0.800 total time=   0.0s\n",
      "[CV 5/5] END C=0.5, coef0=0.01, degree=3, kernel=poly;, score=0.867 total time=   0.0s\n",
      "[CV 1/5] END C=0.5, coef0=1, degree=3, kernel=poly;, score=0.693 total time=   0.0s\n",
      "[CV 2/5] END C=0.5, coef0=1, degree=3, kernel=poly;, score=0.550 total time=   0.0s\n",
      "[CV 3/5] END C=0.5, coef0=1, degree=3, kernel=poly;, score=0.696 total time=   0.0s\n",
      "[CV 4/5] END C=0.5, coef0=1, degree=3, kernel=poly;, score=1.000 total time=   0.0s\n",
      "[CV 5/5] END C=0.5, coef0=1, degree=3, kernel=poly;, score=0.822 total time=   0.0s\n",
      "[CV 1/5] END C=0.5, coef0=0.5, degree=5, kernel=rbf;, score=0.664 total time=   0.0s\n",
      "[CV 2/5] END C=0.5, coef0=0.5, degree=5, kernel=rbf;, score=0.621 total time=   0.0s\n",
      "[CV 3/5] END C=0.5, coef0=0.5, degree=5, kernel=rbf;, score=0.659 total time=   0.0s\n",
      "[CV 4/5] END C=0.5, coef0=0.5, degree=5, kernel=rbf;, score=0.993 total time=   0.0s\n",
      "[CV 5/5] END C=0.5, coef0=0.5, degree=5, kernel=rbf;, score=0.770 total time=   0.0s\n",
      "[CV 1/5] END C=0.5, coef0=0.01, degree=1, kernel=sigmoid;, score=0.557 total time=   0.0s\n",
      "[CV 2/5] END C=0.5, coef0=0.01, degree=1, kernel=sigmoid;, score=0.721 total time=   0.0s\n",
      "[CV 3/5] END C=0.5, coef0=0.01, degree=1, kernel=sigmoid;, score=0.644 total time=   0.0s\n",
      "[CV 4/5] END C=0.5, coef0=0.01, degree=1, kernel=sigmoid;, score=0.570 total time=   0.0s\n",
      "[CV 5/5] END C=0.5, coef0=0.01, degree=1, kernel=sigmoid;, score=0.607 total time=   0.0s\n",
      "[CV 1/5] END C=0.5, coef0=1, degree=5, kernel=poly;, score=0.750 total time=   0.0s\n",
      "[CV 2/5] END C=0.5, coef0=1, degree=5, kernel=poly;, score=0.636 total time=   0.0s\n",
      "[CV 3/5] END C=0.5, coef0=1, degree=5, kernel=poly;, score=0.622 total time=   0.0s\n",
      "[CV 4/5] END C=0.5, coef0=1, degree=5, kernel=poly;, score=0.993 total time=   0.0s\n",
      "[CV 5/5] END C=0.5, coef0=1, degree=5, kernel=poly;, score=0.741 total time=   0.0s\n",
      "[CV 1/5] END C=0.01, coef0=0.01, degree=1, kernel=rbf;, score=0.679 total time=   0.0s\n",
      "[CV 2/5] END C=0.01, coef0=0.01, degree=1, kernel=rbf;, score=0.636 total time=   0.0s\n",
      "[CV 3/5] END C=0.01, coef0=0.01, degree=1, kernel=rbf;, score=0.630 total time=   0.0s\n",
      "[CV 4/5] END C=0.01, coef0=0.01, degree=1, kernel=rbf;, score=0.978 total time=   0.0s\n",
      "[CV 5/5] END C=0.01, coef0=0.01, degree=1, kernel=rbf;, score=0.778 total time=   0.0s\n",
      "[CV 1/5] END C=0.01, coef0=1, degree=5, kernel=rbf;, score=0.679 total time=   0.0s\n",
      "[CV 2/5] END C=0.01, coef0=1, degree=5, kernel=rbf;, score=0.636 total time=   0.0s\n",
      "[CV 3/5] END C=0.01, coef0=1, degree=5, kernel=rbf;, score=0.630 total time=   0.0s\n",
      "[CV 4/5] END C=0.01, coef0=1, degree=5, kernel=rbf;, score=0.978 total time=   0.0s\n",
      "[CV 5/5] END C=0.01, coef0=1, degree=5, kernel=rbf;, score=0.778 total time=   0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'SVC(probability=True)')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = []\n",
    "for train_index, validation_index, in cv_10fold.split(X_train, Y_train):\n",
    "    \n",
    "    X_train_CV = X_train[train_index]\n",
    "    Y_train_CV = Y_train[train_index]\n",
    "\n",
    "    X_validation_CV = X_train[validation_index]\n",
    "    Y_validation_CV = Y_train[validation_index]\n",
    "    \n",
    "    clf = clf_list[4]\n",
    "    parameters = parameters_list[4]\n",
    "    randomized_search = RandomizedSearchCV(clf, parameters, cv=cv_5fold, scoring='roc_auc', n_iter = 10)\n",
    "    randomized_search.fit(X_train_CV, Y_train_CV)\n",
    "    \n",
    "    # Get resulting classifier\n",
    "    clf = randomized_search.best_estimator_\n",
    "\n",
    "    # Test the classifier on the train data\n",
    "    probabilities = clf.predict_proba(X_train_CV)\n",
    "    scores = probabilities[:, 1]\n",
    "    \n",
    "    # Get the auc\n",
    "    auc = metrics.roc_auc_score(Y_train_CV, scores)\n",
    "    results.append({\n",
    "        'auc': auc,\n",
    "        'degree': clf.degree,\n",
    "        'coef0': clf.coef0,\n",
    "        'C': clf.C,\n",
    "        'kernel': clf.kernel,\n",
    "        'set': 'test'\n",
    "    })\n",
    "    \n",
    "    # Test the classifier on the validation data\n",
    "    probabilities_validation = clf.predict_proba(X_validation_CV)\n",
    "    scores_validation = probabilities_validation[:, 1]\n",
    "    \n",
    "    # Get the auc\n",
    "    auc_validation = metrics.roc_auc_score(Y_validation_CV, scores_validation)\n",
    "    results.append({\n",
    "        'auc': auc_validation,\n",
    "        'degree': clf.degree,\n",
    "        'coef0': clf.coef0,\n",
    "        'C': clf.C,\n",
    "        'kernel': clf.kernel,\n",
    "        'set': 'validation'\n",
    "    })\n",
    "    \n",
    "#Create results dataframe and plot it\n",
    "results = pd.DataFrame(results)\n",
    "sns.boxplot(y='auc', x='set', data=results).set_title(f'{clf_list[4]}')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "99c346a2273ffdba9d13565a7efa8a03dbcb19c3f8c63eac036f68118855a3ce"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
