{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to use from colab environment\n",
    "#!pip install -q --upgrade git+https://github.com/naomiverkerk/TM10007.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn import svm\n",
    "from sklearn import decomposition\n",
    "import statistics\n",
    "\n",
    "# Preprocessing\n",
    "from load_data import load_data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import RobustScaler, LabelEncoder\n",
    "\n",
    "# Classifiers and kernels\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector as selector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Import\n",
    "# from sklearn import model_selection\n",
    "# from sklearn import metrics\n",
    "# from sklearn import feature_selection \n",
    "# from sklearn import preprocessing\n",
    "# from sklearn import neighbors\n",
    "# from sklearn import svm\n",
    "# from sklearn import decomposition\n",
    "\n",
    "\n",
    "# from load_data import load_data\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Classifiers and kernels\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.metrics import classification_report\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Loading Data\n",
    "data = load_data() \n",
    "X = data\n",
    "X = X.replace(np.inf, np.nan)\n",
    "Y = data['label']\n",
    "del X['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duplicate check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicated datasets = 0\n",
      "Number of duplicated features = 0\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates values\n",
    "data.drop_duplicates(keep='first')\n",
    "print(f'Number of duplicated datasets = {data.duplicated().sum()}')\n",
    "\n",
    "# Check for duplicates columns\n",
    "data.columns.drop_duplicates(keep='first')\n",
    "print(f'Number of duplicated features = {data.columns.duplicated().sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x396 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "label_count = Y.value_counts()\n",
    "z = np.arange(len(label_count.index))\n",
    "width = 0.75\n",
    "fig, ax = plt.subplots(figsize = (6,5.5))\n",
    "count1 = ax.bar(0.5, label_count.values[0], width, label=label_count.index[0])\n",
    "count2 = ax.bar(1.5, label_count.values[1], width, label=label_count.index[1])\n",
    "\n",
    "ax.set_ylabel('Scores', fontsize = 15)\n",
    "ax.set_title('Number patients per label  ', fontsize = 16)\n",
    "ax.set_xticks([0.5,1.5], list(label_count.index), fontsize = 13)\n",
    "ax.legend()\n",
    "ax.bar_label(count1, padding=3)\n",
    "ax.bar_label(count2, padding=3)\n",
    "ax.grid(False)\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split\n",
    "X_train, X_test, Y_train, Y_test= train_test_split(X, Y, test_size = 0.2, random_state = 4, stratify = Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Nog ff naar kijken of het werkt\n",
    "# from fitter import Fitter, get_common_distributions, get_distributions\n",
    "# fi = []\n",
    "\n",
    "# col = X.columns\n",
    "# for i in (range(0,X.shape[1],1)):\n",
    "#     try:\n",
    "#         X_tr = X[col[i]].values\n",
    "#         f = Fitter(X_tr,\n",
    "#             distributions=['gamma',\n",
    "#                             'lognorm',\n",
    "#                             \"beta\",\n",
    "#                             \"burr\",\n",
    "#                             \"norm\"])\n",
    "#         f.fit()\n",
    "#         fi.append(list(pd.DataFrame(f.get_best(method = 'sumsquare_error')).columns))\n",
    "#     except Exception:\n",
    "#         pass\n",
    "\n",
    "# flat_list = [item for sublist in fi for item in sublist]\n",
    "# from collections import Counter\n",
    "# Counter(flat_list).most_common()       \n",
    "# from sklearn import preprocessing\n",
    "# import seaborn as sns\n",
    "# import matplotlib.patches as mpatches\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# le = preprocessing.LabelEncoder()\n",
    "\n",
    "# y_10 = le.fit_transform(Y)\n",
    "# Y_train_10 = le.fit_transform(Y_train)\n",
    "# Y_test_10 = le.fit_transform(Y_test)\n",
    "# # classes = data.label\n",
    "# # classes = list(classes.unique())\n",
    "# colour = sns.color_palette(\"Set2\")\n",
    "# color1=colour[0]\n",
    "# color2=colour[1]\n",
    "\n",
    "# colormap = np.array([color1,color2])\n",
    "\n",
    "# fig = plt.figure(figsize=(24,8))\n",
    "# ax = fig.add_subplot(131)\n",
    "# ax.set_title(f\"Brats, entire dataset = {len(data)}\", fontsize=15)\n",
    "# ax.scatter(data.iloc[:,0], data.iloc[:,1], marker='o', c=colormap[y_10],\n",
    "#            s=45, edgecolor='k', cmap=plt.cm.Paired)\n",
    "\n",
    "# ax = fig.add_subplot(132)\n",
    "# ax.set_title(f\"Training data = {len(X_train)}\", fontsize=15)\n",
    "# ax.scatter(X_train.iloc[:,0], X_train.iloc[:,1], marker='o', c=colormap[Y_train_10],\n",
    "#            s=45, edgecolor='k', cmap=plt.cm.Paired)\n",
    "\n",
    "# ax = fig.add_subplot(133)\n",
    "# ax.set_title(f\"Test data= {len(X_test)}\", fontsize=15)\n",
    "# ax.scatter(X_test.iloc[:,0], X_test.iloc[:,1], marker='o', c=colormap[Y_test_10],\n",
    "#            s=45, edgecolor='k', cmap=plt.cm.Paired)   \n",
    "\n",
    "# GBM_patch = mpatches.Patch(color=colour[0], label='GBM')\n",
    "# LGG_patch = mpatches.Patch(color=colour[1], label='LGG')\n",
    "# fig.legend(handles=[GBM_patch, LGG_patch],loc=\"center right\", prop={'size': 12})\n",
    "# fig.subplots_adjust(right=0.95)\n",
    "\n",
    "# plt.show()          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Features weghalen met teveel missing values\n",
    "acceptabele_ratio = 0.5\n",
    "train_size = len(X_train.index)\n",
    "removal_rate = round(train_size*acceptabele_ratio)\n",
    "\n",
    "X_train = X_train.dropna(axis=1, thresh=removal_rate)\n",
    "common_cols = list(set(X_train.columns).intersection(X_test.columns))\n",
    "X_test = X_test[common_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sara\\miniconda3\\lib\\site-packages\\sklearn\\base.py:493: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
      "Feature names must be in the same order as they were in fit.\n",
      "\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "imputer = KNNImputer()\n",
    "imputer.fit(X_train)\n",
    "X_train_imputer = imputer.transform(X_train)\n",
    "X_test_imputer = imputer.transform(X_test)\n",
    "\n",
    "# np.isnan(X_train_imputer).sum()\n",
    "# np.isnan(X_test_imputer).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Imputation met median\n",
    "# X_train_missing_median = X_train.fillna(X_train.median())\n",
    "# X_train_missing_median = np.nan_to_num(X_train_missing_median)\n",
    "# X_test_missing_median = X_test.fillna(X_test.median())\n",
    "# X_test_missing_median = np.nan_to_num(X_test_missing_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scaling\n",
    "scaler = RobustScaler()\n",
    "scaler.fit(X_train_imputer)\n",
    "X_train_scaled = scaler.transform(X_train_imputer)\n",
    "X_test_scaled = scaler.transform(X_test_imputer)\n",
    "\n",
    "# X_train_scaled = scaler.transform(X_train_missing_median)\n",
    "# X_train_scaled = np.nan_to_num(X_train_scaled)\n",
    "# X_test_scaled = scaler.transform(X_test_missing_median)\n",
    "# X_test_scaled = np.nan_to_num(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## scaling van y\n",
    "scaler = LabelEncoder()\n",
    "scaler.fit(Y_train)\n",
    "Y_train_final = scaler.transform(Y_train)\n",
    "Y_test_final = scaler.transform(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PCA\n",
    "pca = decomposition.PCA(n_components=0.99, svd_solver= 'full')\n",
    "pca.fit(X_train_scaled)\n",
    "X_train_final = pca.transform(X_train_scaled)\n",
    "X_test_final = pca.transform(X_test_scaled)\n",
    "# explained_variance = np.cumsum(pca.explained_variance_ratio_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan in x train final 0, nan in x train final0\n"
     ]
    }
   ],
   "source": [
    "print(f'nan in x train final {np.isnan(X_train_final).sum()}, nan in x train final{np.isnan(X_test_final).sum()}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_10fold = StratifiedKFold(n_splits=5)\n",
    "cv_5fold = StratifiedKFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# results = []\n",
    "# best_min_samples_split = []\n",
    "\n",
    "# for validation_index, train_index in cv_10fold.split(X_train_final, Y_train_final):\n",
    "\n",
    "#         # Split the data properly\n",
    "#         X_validation_CV = X_train_final[validation_index]\n",
    "#         Y_validation_CV = Y_train_final[validation_index]\n",
    "        \n",
    "#         X_train_CV = X_train_final[train_index]\n",
    "#         Y_train_CV = Y_train_final[train_index]\n",
    "\n",
    "#         parameters = {\n",
    "#                         \"criterion\": ['gini', 'entropy'],\n",
    "#                         \"min_samples_split\": list(range(2,40,2)),\n",
    "#                         # \"min_impurity_decrease\": [0,1,2,3,4,5],\n",
    "#                         \"max_features\": [1,2,3,4,5],\n",
    "#                         \"min_samples_leaf\": list(range(1,20,2)),\n",
    "#         }\n",
    "#         clf = RandomForestClassifier()\n",
    "#         cv_5fold = StratifiedKFold(n_splits=5)\n",
    "#         grid_search = GridSearchCV(clf, parameters, cv=cv_5fold, scoring='roc_auc')\n",
    "#         grid_search.fit(X_validation_CV, Y_validation_CV)\n",
    "        \n",
    "#         # Get resulting classifier\n",
    "#         clf = grid_search.best_estimator_\n",
    "#         print(f'Best classifier for criterion={clf.criterion} & min_samples_split={clf.min_samples_split} & max_features={clf.max_features} & min_samples_leaf={clf.min_samples_leaf}')\n",
    "\n",
    "#         probabilities = clf.predict_proba(X_test)\n",
    "#         scores = probabilities[:, 1]\n",
    "        \n",
    "#         auc = metrics.roc_auc_score(Y_train_CV, scores)\n",
    "#         results.append({\n",
    "#             'auc': auc,\n",
    "#             'criterion': clf.criterion,\n",
    "#             'min_samples_split': clf.min_samples_split,\n",
    "#             'max_features': clf.max_features,\n",
    "#             \"min_samples_leaf\": clf.min_samples_leaf,\n",
    "#             'set': 'test'\n",
    "#         })\n",
    "        \n",
    "#         probabilities_validation = clf.predict_proba(X_validation_CV)\n",
    "#         scores_validation = probabilities_validation[:, 1]\n",
    "\n",
    "#         # Get the auc\n",
    "#         auc_validation = metrics.roc_auc_score(Y_validation_CV, scores_validation)\n",
    "#         results.append({\n",
    "#             'auc': auc_validation,\n",
    "#             'criterion': clf.criterion,\n",
    "#             'min_samples_split': clf.min_samples_split,\n",
    "#             'max_features': clf.max_features,\n",
    "#             \"min_samples_leaf\": clf.min_samples_leaf,\n",
    "#             'set': 'validation'\n",
    "#         })\n",
    "\n",
    "#     # plt.figure(dpi=250)\n",
    "#     # plot_tree(clf, filled=True,\n",
    "#     #             class_names=class_names,\n",
    "#     #             feature_names=feature_names,\n",
    "#     #             );\n",
    "\n",
    "\n",
    "\n",
    "# results = pd.DataFrame(results)\n",
    "# sns.boxplot(y='auc', x='set', data=results)\n",
    "\n",
    "# p = list(parameters.keys())\n",
    "# optimal_parameter = []\n",
    "# # print(f\"The optimal N={optimal_n}\")\n",
    "\n",
    "# parameter_keys = list(parameters.keys())\n",
    "# for item in parameter_keys:\n",
    "#     best_item = [] \n",
    "#     for i in list(range(0,10,2)):\n",
    "#         best_item.append(results[item][i])\n",
    "\n",
    "#     optimal_parameter.append(statistics.median(best_item))\n",
    "#     print(f\"The optimal {item}={optimal_parameter[-1]}\")\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "    #     # Split the data properly\n",
    "    #     X_validation = X_train_final[validation_index]\n",
    "    #     y_validation = Y_train_final[validation_index]\n",
    "        \n",
    "    #     X_test = X_train_final[test_index]\n",
    "    #     y_test = Y_train_final[test_index]\n",
    "\n",
    "    #     parameters = {\n",
    "    #                     \"criterion\": ['gini', 'entropy'],\n",
    "    #                     \"min_samples_split\": list(range(2,40,2)),\n",
    "    #                     # \"min_impurity_decrease\": [0,1,2,3,4,5],\n",
    "    #                     \"max_features\": [1,2,3,4,5],\n",
    "    #                     \"min_samples_leaf\": list(range(1,20,2)),\n",
    "    #     }\n",
    "    #     clf = RandomForestClassifier()\n",
    "    #     cv_10fold = model_selection.StratifiedKFold(n_splits=5)\n",
    "    #     grid_search = model_selection.GridSearchCV(clf, parameters, cv=cv_10fold, scoring='roc_auc')\n",
    "    #     grid_search.fit(X_validation, y_validation)\n",
    "        \n",
    "    #     # Get resulting classifier\n",
    "    #     clf = grid_search.best_estimator_\n",
    "    #     print(f'Best classifier for criterion={clf.criterion} & min_samples_split={clf.min_samples_split} & max_features={clf.max_features} & min_samples_leaf={clf.min_samples_leaf}')\n",
    "\n",
    "    #     probabilities = clf.predict_proba(X_test)\n",
    "    #     scores = probabilities[:, 1]\n",
    "        \n",
    "    #     auc = metrics.roc_auc_score(y_test, scores)\n",
    "    #     results.append({\n",
    "    #         'auc': auc,\n",
    "    #         'criterion': clf.criterion,\n",
    "    #         'min_samples_split': clf.min_samples_split,\n",
    "    #         'max_features': clf.max_features,\n",
    "    #         \"min_samples_leaf\": clf.min_samples_leaf,\n",
    "    #         'set': 'test'\n",
    "    #     })\n",
    "        \n",
    "    #     probabilities_validation = clf.predict_proba(X_validation)\n",
    "    #     scores_validation = probabilities_validation[:, 1]\n",
    "\n",
    "    #     # Get the auc\n",
    "    #     auc_validation = metrics.roc_auc_score(y_validation, scores_validation)\n",
    "    #     results.append({\n",
    "    #         'auc': auc_validation,\n",
    "    #         'criterion': clf.criterion,\n",
    "    #         'min_samples_split': clf.min_samples_split,\n",
    "    #         'max_features': clf.max_features,\n",
    "    #         \"min_samples_leaf\": clf.min_samples_leaf,\n",
    "    #         'set': 'validation'\n",
    "    #     })\n",
    "\n",
    "    # # plt.figure(dpi=250)\n",
    "    # # plot_tree(clf, filled=True,\n",
    "    # #             class_names=class_names,\n",
    "    # #             feature_names=feature_names,\n",
    "    # #             );\n",
    "\n",
    "\n",
    "\n",
    "    # results = pd.DataFrame(results)\n",
    "    # sns.boxplot(y='auc', x='set', data=results)\n",
    "\n",
    "    # p = list(parameters.keys())\n",
    "    # optimal_parameter = []\n",
    "    # # print(f\"The optimal N={optimal_n}\")\n",
    "\n",
    "    # parameter_keys = list(parameters.keys())\n",
    "    # for item in parameter_keys:\n",
    "    #     best_item = [] \n",
    "    #     for i in list(range(0,10,2)):\n",
    "    #         best_item.append(results[item][i])\n",
    "\n",
    "    #     optimal_parameter.append(statistics.median(best_item))\n",
    "    #     print(f\"The optimal {item}={optimal_parameter[-1]}\")\n",
    "  \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv_10fold = StratifiedKFold(n_splits=10)\n",
    "# results = []\n",
    "# best_min_samples_split = []\n",
    "\n",
    "# for validation_index, train_index in cv_10fold.split(X_train_final, Y_train_final):\n",
    "\n",
    "#         # Split the data properly\n",
    "#         X_validation_CV = X_train_final[validation_index]\n",
    "#         Y_validation_CV = Y_train_final[validation_index]\n",
    "        \n",
    "#         X_train_CV = X_train_final[train_index]\n",
    "#         Y_train_CV = Y_train_final[train_index]\n",
    "\n",
    "#         parameters = {\n",
    "#                         \"criterion\": ['gini', 'entropy'],\n",
    "#                         \"min_samples_split\": list(range(2,40,2)),\n",
    "#                         # \"min_impurity_decrease\": [0,1,2,3,4,5],\n",
    "#                         \"max_features\": [1,2,3,4,5],\n",
    "#                         \"min_samples_leaf\": list(range(1,20,2)),\n",
    "#         }\n",
    "#         clf = RandomForestClassifier()\n",
    "#         cv_5fold = StratifiedKFold(n_splits=5)\n",
    "#         grid_search = GridSearchCV(clf, parameters, cv=cv_5fold, scoring='roc_auc')\n",
    "#         grid_search.fit(X_validation_CV, Y_validation_CV)\n",
    "        \n",
    "#         # Get resulting classifier\n",
    "#         clf = grid_search.best_estimator_\n",
    "#         print(f'Best classifier for criterion={clf.criterion} & min_samples_split={clf.min_samples_split} & max_features={clf.max_features} & min_samples_leaf={clf.min_samples_leaf}')\n",
    "\n",
    "#         probabilities = clf.predict_proba(X_test)\n",
    "#         scores = probabilities[:, 1]\n",
    "        \n",
    "#         auc = metrics.roc_auc_score(Y_train_CV, scores)\n",
    "#         results.append({\n",
    "#             'auc': auc,\n",
    "#             'criterion': clf.criterion,\n",
    "#             'min_samples_split': clf.min_samples_split,\n",
    "#             'max_features': clf.max_features,\n",
    "#             \"min_samples_leaf\": clf.min_samples_leaf,\n",
    "#             'set': 'test'\n",
    "#         })\n",
    "        \n",
    "#         probabilities_validation = clf.predict_proba(X_validation_CV)\n",
    "#         scores_validation = probabilities_validation[:, 1]\n",
    "\n",
    "#         # Get the auc\n",
    "#         auc_validation = metrics.roc_auc_score(Y_validation_CV, scores_validation)\n",
    "#         results.append({\n",
    "#             'auc': auc_validation,\n",
    "#             'criterion': clf.criterion,\n",
    "#             'min_samples_split': clf.min_samples_split,\n",
    "#             'max_features': clf.max_features,\n",
    "#             \"min_samples_leaf\": clf.min_samples_leaf,\n",
    "#             'set': 'validation'\n",
    "#         })\n",
    "\n",
    "#     # plt.figure(dpi=250)\n",
    "#     # plot_tree(clf, filled=True,\n",
    "#     #             class_names=class_names,\n",
    "#     #             feature_names=feature_names,\n",
    "#     #             );\n",
    "\n",
    "\n",
    "\n",
    "# results = pd.DataFrame(results)\n",
    "# sns.boxplot(y='auc', x='set', data=results)\n",
    "\n",
    "# p = list(parameters.keys())\n",
    "# optimal_parameter = []\n",
    "# # print(f\"The optimal N={optimal_n}\")\n",
    "\n",
    "# parameter_keys = list(parameters.keys())\n",
    "# for item in parameter_keys:\n",
    "#     best_item = [] \n",
    "#     for i in list(range(0,10,2)):\n",
    "#         best_item.append(results[item][i])\n",
    "\n",
    "#     optimal_parameter.append(statistics.median(best_item))\n",
    "#     print(f\"The optimal {item}={optimal_parameter[-1]}\")\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-nearest neighbour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sara\\miniconda3\\lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but KNeighborsClassifier was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19320/1520518223.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;31m# Test the classifier on the test data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     \u001b[0mprobabilities\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m     \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprobabilities\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    254\u001b[0m             \u001b[0mby\u001b[0m \u001b[0mlexicographic\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m         \"\"\"\n\u001b[1;32m--> 256\u001b[1;33m         \u001b[0mneigh_dist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneigh_ind\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m         \u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\sklearn\\neighbors\\_base.py\u001b[0m in \u001b[0;36mkneighbors\u001b[1;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[0;32m    715\u001b[0m                 \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_precomputed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    716\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 717\u001b[1;33m                 \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csr\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    718\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    719\u001b[0m             \u001b[0mquery_is_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    564\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Validation should be done on X, y or both.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    565\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 566\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    567\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    568\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    798\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    799\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 800\u001b[1;33m             \u001b[0m_assert_all_finite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mforce_all_finite\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"allow-nan\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    801\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    802\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[0;32m    112\u001b[0m         ):\n\u001b[0;32m    113\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"infinity\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"NaN, infinity\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m    115\u001b[0m                 msg_err.format(\n\u001b[0;32m    116\u001b[0m                     \u001b[0mtype_err\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg_dtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mmsg_dtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "results = []\n",
    "best_n_neighbors = []\n",
    "n_components = []\n",
    "\n",
    "\n",
    "for validation_index, train_index in cv_10fold.split(X_train_final, Y_train_final):\n",
    "    # Split the data properly\n",
    "    X_validation_CV = X_train_final[validation_index]\n",
    "    Y_validation_CV = Y_train[validation_index]\n",
    "\n",
    "    X_train_CV = X_train_final[train_index]\n",
    "    Y_train_CV = Y_train[train_index]\n",
    "\n",
    "    # Create a grid search to find the optimal k using a gridsearch and 10-fold cross validation\n",
    "    # Same as above\n",
    "    parameters = {\n",
    "                    \"n_neighbors\": list(range(1, 26, 2)),\n",
    "                    \"weights\": [\"uniform\", \"distance\"],\n",
    "                    \"metric\": [\"euclidean\", \"manhattan\", \"chebyshev\", \"minkowski\"]\n",
    "\n",
    "    }\n",
    "    \n",
    "    clf = KNeighborsClassifier()\n",
    "    grid_search = GridSearchCV(clf, parameters, cv=cv_10fold, scoring='roc_auc')\n",
    "    grid_search.fit(X_validation_CV, Y_validation_CV)\n",
    "    \n",
    "    # Get resulting classifier\n",
    "    clf = grid_search.best_estimator_\n",
    "\n",
    "    # Test the classifier on the test data\n",
    "    probabilities = clf.predict_proba(X_test)\n",
    "    scores = probabilities[:, 1]\n",
    "    \n",
    "    # Get the auc\n",
    "    auc = metrics.roc_auc_score(Y_train_CV, scores)\n",
    "    results.append({\n",
    "        'auc': auc,\n",
    "        'k': clf.n_neighbors,\n",
    "        \"weights\": clf.weights,\n",
    "        \"metric\": clf.metric,\n",
    "        'set': 'test'\n",
    "    })\n",
    "    \n",
    "    # Test the classifier on the validation data\n",
    "    probabilities_validation = clf.predict_proba(X_validation_CV)\n",
    "    scores_validation = probabilities_validation[:, 1]\n",
    "    \n",
    "    # Get the auc\n",
    "    auc_validation = metrics.roc_auc_score(Y_validation_CV, scores_validation)\n",
    "    results.append({\n",
    "        'auc': auc_validation,\n",
    "        'k': clf.n_neighbors,\n",
    "        \"weights\": clf.weights,\n",
    "        \"metric\": clf.metric,\n",
    "        'set': 'validation'\n",
    "    })\n",
    "    \n",
    "# Create results dataframe and plot it\n",
    "results = pd.DataFrame(results)\n",
    "sns.boxplot(y='auc', x='set', data=results)\n",
    "\n",
    "optimal_parameter = []\n",
    "parameter_keys = list(parameters.keys())\n",
    "\n",
    "for item in parameter_keys:\n",
    "    best_item = [] \n",
    "    for i in list(range(0,10,2)):\n",
    "        best_item.append(results[item][i])\n",
    "    \n",
    "    optimal_parameter.append(statistics.median(best_item))\n",
    "    print(f\"The optimal {item}={optimal_parameter[-1]}\")\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# import seaborn as sns\n",
    "# import statistics\n",
    "\n",
    "# results = []\n",
    "# best_n_neighbors = []\n",
    "# n_components = []\n",
    "\n",
    "\n",
    "# # Loop over the folds\n",
    "# for validation_index, test_index in cv_10fold.split(X_train_final, Y_train_final):\n",
    "#     # Split the data properly\n",
    "#     X_validation = X_train_final[validation_index]\n",
    "#     y_validation = Y_train_final[validation_index]\n",
    "\n",
    "#     X_test = X_train_final[test_index]\n",
    "#     y_test = Y_train_final[test_index]\n",
    "\n",
    "#     parameters = {\n",
    "#                     \"solver\": ['newton-cg', 'lbfgs', 'liblinear'],\n",
    "#                     \"C\": [100, 10, 1.0, 0.1, 0.01],\n",
    "#     }\n",
    "\n",
    "#     lg = LogisticRegression()\n",
    "#     cv_10fold = model_selection.StratifiedKFold(n_splits=5)\n",
    "#     grid_search = model_selection.GridSearchCV(lg, parameters, cv=cv_10fold, scoring='roc_auc')\n",
    "#     grid_search.fit(X_validation, y_validation)\n",
    "\n",
    "#     clf = grid_search.best_estimator_\n",
    "\n",
    "#     # Test the classifier on the test data\n",
    "#     probabilities = clf.predict_proba(X_test)\n",
    "#     scores = probabilities[:, 1]\n",
    "    \n",
    "#     # Get the auc\n",
    "#     auc = metrics.roc_auc_score(y_test, scores)\n",
    "#     results.append({\n",
    "#         'auc': auc,\n",
    "#         \"solver\": clf.solver,\n",
    "#         \"C\": clf.C,\n",
    "#         'set': 'test'\n",
    "#     })\n",
    "    \n",
    "#     # Test the classifier on the validation data\n",
    "#     probabilities_validation = clf.predict_proba(X_validation)\n",
    "#     scores_validation = probabilities_validation[:, 1]\n",
    "    \n",
    "#     # Get the auc\n",
    "#     auc_validation = metrics.roc_auc_score(y_validation, scores_validation)\n",
    "#     results.append({\n",
    "#         'auc': auc,\n",
    "#         \"solver\": clf.solver,\n",
    "#         \"C\": clf.C,\n",
    "#         'set': 'validation'\n",
    "#     })\n",
    "    \n",
    "# # Create results dataframe and plot it\n",
    "# results = pd.DataFrame(results)\n",
    "# sns.boxplot(y='auc', x='set', data=results)\n",
    "\n",
    "# optimal_parameter = []\n",
    "# parameter_keys = list(parameters.keys())\n",
    "\n",
    "# for item in parameter_keys:\n",
    "#     best_item = [] \n",
    "#     for i in list(range(0,10,2)):\n",
    "#         best_item.append(results[item][i])\n",
    "    \n",
    "#     optimal_parameter.append(statistics.median(best_item))\n",
    "#     print(f\"The optimal {item}={optimal_parameter[-1]}\")\n",
    "\n",
    "# results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naives Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# # # model = MultinomialNB().fit(X_train, y_train)\n",
    "\n",
    "# best_var_smoothing = []\n",
    "# results = []\n",
    "\n",
    "# cv_20fold = model_selection.StratifiedKFold(n_splits=5)\n",
    "\n",
    "# # Loop over the folds\n",
    "# for validation_index, test_index in cv_20fold.split(X_train_final, Y_train_final):\n",
    "\n",
    "#     # Split the data properly\n",
    "#     X_validation = X_train_final[validation_index]\n",
    "#     y_validation = Y_train_final[validation_index]\n",
    "    \n",
    "#     X_test = X_train_final[test_index]\n",
    "#     y_test = Y_train_final[test_index]\n",
    "\n",
    "#     parameters = {\"var_smoothing\": list(np.logspace(0,-9, num=101))}\n",
    "#     gnb = GaussianNB()\n",
    "#     cv_10fold = model_selection.StratifiedKFold(n_splits=5)\n",
    "#     grid_search = model_selection.GridSearchCV(gnb, parameters, cv=cv_10fold, scoring='roc_auc')\n",
    "#     grid_search.fit(X_validation, y_validation)\n",
    "    \n",
    "#     # Get resulting classifier\n",
    "#     clf = grid_search.best_estimator_\n",
    "#     print(f'Best classifier for parameter={clf.var_smoothing}')\n",
    "#     best_var_smoothing.append(clf.var_smoothing)\n",
    "    \n",
    "#     # Test the classifier on the test data\n",
    "#     probabilities = clf.predict_proba(X_test)\n",
    "#     scores = probabilities[:, 1]\n",
    "    \n",
    "#     # Get the auc\n",
    "#     auc = metrics.roc_auc_score(y_test, scores)\n",
    "#     results.append({\n",
    "#         'auc': auc,\n",
    "#         'k': clf.var_smoothing,\n",
    "#         'set': 'test'\n",
    "#     })\n",
    "    \n",
    "#     # Test the classifier on the validation data\n",
    "#     probabilities_validation = clf.predict_proba(X_validation)\n",
    "#     scores_validation = probabilities_validation[:, 1]\n",
    "    \n",
    "#     # Get the auc\n",
    "#     auc_validation = metrics.roc_auc_score(y_validation, scores_validation)\n",
    "#     results.append({\n",
    "#         'auc': auc_validation,\n",
    "#         'k': clf.var_smoothing,\n",
    "#         'set': 'validation'\n",
    "#     })\n",
    "    \n",
    "# # Create results dataframe and plot it\n",
    "# results = pd.DataFrame(results)\n",
    "# sns.boxplot(y='auc', x='set', data=results)\n",
    "\n",
    "# optimal_var_smoothing = (np.median(best_var_smoothing))\n",
    "# print(f\"The optimal value of parameter={optimal_var_smoothing}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.neural_network import MLPClassifier\n",
    "# import warnings\n",
    "# import seaborn as sns\n",
    "# import statistics\n",
    "# # Create a 20 fold stratified CV iterator\n",
    "\n",
    "# cv_20fold = model_selection.StratifiedKFold(n_splits=5)\n",
    "# results = []\n",
    "# best_n_neighbors = []\n",
    "# n_components = []\n",
    "\n",
    "# # Loop over the folds\n",
    "# for validation_index, test_index in cv_20fold.split(X_train_final, Y_train_final):\n",
    "#     # Split the data properly\n",
    "#     X_validation = X_train_final[validation_index]\n",
    "#     y_validation = Y_train_final[validation_index]\n",
    "\n",
    "#     X_test = X_train_final[test_index]\n",
    "#     y_test = Y_train_final[test_index]\n",
    "\n",
    "#     parameters = {\n",
    "#                     'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)],\n",
    "#                     'activation': ['tanh', 'relu'],\n",
    "#                     'solver': ['sgd', 'adam'],\n",
    "#                     'alpha': [0.0001, 0.05],\n",
    "#                     'learning_rate': ['constant','adaptive'],\n",
    "#     }\n",
    "\n",
    "#     clf = MLPClassifier(random_state = 1)\n",
    "#     cv_10fold = model_selection.StratifiedKFold(n_splits=5)\n",
    "#     grid_search = model_selection.GridSearchCV(clf, parameters, cv=cv_10fold, scoring='roc_auc')\n",
    "#     grid_search.fit(X_validation, y_validation)\n",
    "\n",
    "#     clf = grid_search.best_estimator_\n",
    "\n",
    "#     # Test the classifier on the test data\n",
    "#     probabilities = clf.predict_proba(X_test)\n",
    "#     scores = probabilities[:, 1]\n",
    "    \n",
    "#     # Get the auc\n",
    "#     auc = metrics.roc_auc_score(y_test, scores)\n",
    "#     results.append({\n",
    "#         'auc': auc,\n",
    "#         \"hidden layer sizes\": clf.hidden_layer_size,\n",
    "#         \"activation\": clf.activation,\n",
    "#         \"solver\": clf.solver,\n",
    "#         \"alpha\": clf.alpha,\n",
    "#         \"learning rate\": clf.learning_rate,\n",
    "#         'set': 'test'\n",
    "#     })\n",
    "    \n",
    "#     # Test the classifier on the validation data\n",
    "#     probabilities_validation = clf.predict_proba(X_validation)\n",
    "#     scores_validation = probabilities_validation[:, 1]\n",
    "    \n",
    "#     # Get the auc\n",
    "#     auc_validation = metrics.roc_auc_score(y_validation, scores_validation)\n",
    "#     results.append({\n",
    "#         'auc': auc,\n",
    "#         \"hidden layer sizes\": clf.hidden_layer_size,\n",
    "#         \"activation\": clf.activation,\n",
    "#         \"solver\": clf.solver,\n",
    "#         \"alpha\": clf.alpha,\n",
    "#         \"learning rate\": clf.learning_rate,\n",
    "#         'set': 'validation'\n",
    "#     })\n",
    "    \n",
    "# # Create results dataframe and plot it\n",
    "# results = pd.DataFrame(results)\n",
    "# sns.boxplot(y='auc', x='set', data=results)\n",
    "\n",
    "# optimal_parameter = []\n",
    "# parameter_keys = list(parameters.keys())\n",
    "\n",
    "# for item in parameter_keys:\n",
    "#     best_item = [] \n",
    "#     for i in list(range(0,10,2)):\n",
    "#         best_item.append(results[item][i])\n",
    "    \n",
    "#     optimal_parameter.append(statistics.median(best_item))\n",
    "#     print(f\"The optimal {item}={optimal_parameter[-1]}\")\n",
    "\n",
    "# results"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "99c346a2273ffdba9d13565a7efa8a03dbcb19c3f8c63eac036f68118855a3ce"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
